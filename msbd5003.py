# -*- coding: utf-8 -*-
"""Copy of MSBD5003 Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bOOh5PJpaf9p7q4FbsErO1QwIMzNd_x8

# 1. Environment Setup

### 1.2 Init Spark and Load Dataset
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import monotonically_increasing_id

# Step 1: Initialize SparkSession
spark = SparkSession.builder \
    .appName("Text Processing Pipeline") \
    .getOrCreate()

path = "df_file.csv"

df = spark.read \
  .option("header", True) \
  .option("inferSchema", True) \
  .option("multiLine", True) \
  .option("escape", '"') \
.csv(path).withColumn("doc_id", monotonically_increasing_id())
df.show()

N = df.count()
print("N = ", N)

import re

# Initialize Spark session
spark = SparkSession.builder.appName("MergeNewsgroupFiles").getOrCreate()
sc = spark.sparkContext

"""# 2. Tokenization"""

from pyspark.sql.functions import udf, desc_nulls_last
from pyspark.sql.types import ArrayType, StringType, DoubleType
import re
import nltk
from nltk.corpus import stopwords
from nltk import download
download('stopwords')
STOPWORDS = set(stopwords.words('english'))

def tokenize(doc):
  punctuation_removed = re.sub(r'[^a-zA-Z0-9\s]', '', doc.lower())
  tokenized = re.split(r'[\s\n]+', punctuation_removed)
  return [w for w in tokenized if w not in stopwords.words('english') and len(w) > 0]

tokenize_udf = udf(tokenize, ArrayType(StringType()))

tokenized_rdd = df.select(df["doc_id"], tokenize_udf(df["Text"])).rdd.map(lambda x: (x[0], x[1]))
tokenized_rdd.take(5)

"""# 3. TF/IDF Pipeline"""

from collections import Counter

bag_of_words = tokenized_rdd.mapValues(lambda x: (Counter(x), len(x)))

global_tf = bag_of_words.flatMap(lambda x: [(word, x[0], local_count, local_count / x[1][1]) for word, local_count in x[1][0].items()])
tf_df = global_tf.toDF(["token", "doc_id", "local_count", "tf"])
tf_df.cache()
tf_df.printSchema()

token_global_df = tf_df.groupBy("token").count()

idf = udf(lambda x: N / x, DoubleType())

idf_df = token_global_df.withColumn("idf", idf(token_global_df["count"])).select("token", "idf")
idf_df.cache()

tf_idf = udf(lambda tf, idf: tf * idf, DoubleType())

tf_idf_df = tf_df.join(idf_df, "token")
main_df = tf_idf_df.withColumn("tf_idf", tf_idf(tf_idf_df["tf"], tf_idf_df["idf"])).select("token", "doc_id", "local_count", "tf_idf")
main_df.cache()
main_df.sort(desc_nulls_last("tf_idf")).show()