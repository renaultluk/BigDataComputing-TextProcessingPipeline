{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvn1P2j3Eawu"
   },
   "source": [
    "# 1. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjoBhX6UJdCY"
   },
   "source": [
    "### 1.1 Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-y2ALmYnIhtx"
   },
   "source": [
    "### 1.2 Init Spark and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "9hTH0yQQV7nG"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, collect_list, struct, monotonically_increasing_id\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType, StructType, StructField\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: df_Mid_Size.csv\n"
     ]
    }
   ],
   "source": [
    "path=\"df_Mid_Size.csv\"\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QyoQFlmXEduv"
   },
   "outputs": [],
   "source": [
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Text Processing Pipeline\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkjP4EY_Iu5m",
    "outputId": "e66a3fc1-578b-4d0d-a8df-9981c7511fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|doc_id|                Text|\n",
      "+------+--------------------+\n",
      "|     0|CALL FOR PRESENTA...|\n",
      "|     1|In article <1993M...|\n",
      "|     2|Geoffrey S. Elbo ...|\n",
      "|     3|>     There's one...|\n",
      "|     4|jorge@erex.East.S...|\n",
      "|     5|In article <1993A...|\n",
      "|     6|In article <C51H9...|\n",
      "|     7|In <sehari.734022...|\n",
      "|     8|I am looking for ...|\n",
      "|     9|>ALL icons in Pro...|\n",
      "|    10|>Well, you may th...|\n",
      "|    11|[ Article crosspo...|\n",
      "|    12|In article <1ppmv...|\n",
      "|    13|In article <C4tDG...|\n",
      "|    14|In article <1993A...|\n",
      "|    15|In article <sehar...|\n",
      "|    16|f_langleyrh@ccsva...|\n",
      "|    17|In article <C528H...|\n",
      "|    18|In article <1pq66...|\n",
      "|    19|Has anyone else e...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 15:46:19 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# Load DataFrame and clean null/empty Text rows\n",
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .option(\"escape\", '\"') \\\n",
    "    .csv(path) \\\n",
    "    .filter(col(\"Text\").isNotNull() & (col(\"Text\") != \"\")) \\\n",
    "    .withColumn(\"doc_id\", monotonically_increasing_id())\n",
    "df.cache()  # Cache DataFrame to avoid recomputation\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6MW9gx0fv5b",
    "outputId": "d45d980a-6065-48f5-c258-33ce9b88731a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  18267\n"
     ]
    }
   ],
   "source": [
    "N = df.count()\n",
    "print(\"N = \", N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoufMChsMzkw"
   },
   "source": [
    "# 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWMyumemPmRt",
    "outputId": "d6394aa6-00aa-4c13-dfa4-d528678cfb0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/geoffreychu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# Download and load stopwords once\n",
    "download('stopwords')  # Ensures data exists\n",
    "STOPWORDS = set(stopwords.words('english'))  # Load into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "IVnSKI1wPtQQ"
   },
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    if doc is None or not isinstance(doc, str) or doc.strip() == \"\":\n",
    "        return []\n",
    "    punctuation_removed = re.sub(r'[^a-zA-Z0-9\\s]', '', doc.lower())\n",
    "    tokenized = re.split(r'[\\s\\n]+', punctuation_removed)\n",
    "    return [w for w in tokenized if w not in STOPWORDS and len(w) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Ej8m38LyQbEt"
   },
   "outputs": [],
   "source": [
    "tokenize_udf = udf(tokenize, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZixvOcqUBOa",
    "outputId": "52a0143b-d5cf-4343-f495-799756a5e684"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 15:46:21 WARN BlockManager: Task 623 already completed, not releasing lock for rdd_116_0\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ['call',\n",
       "   'presentations',\n",
       "   'navy',\n",
       "   'scientific',\n",
       "   'visualization',\n",
       "   'virtual',\n",
       "   'reality',\n",
       "   'seminar',\n",
       "   'tuesday',\n",
       "   'june',\n",
       "   '22',\n",
       "   '1993',\n",
       "   'carderock',\n",
       "   'division',\n",
       "   'naval',\n",
       "   'surface',\n",
       "   'warfare',\n",
       "   'center',\n",
       "   'formerly',\n",
       "   'david',\n",
       "   'taylor',\n",
       "   'research',\n",
       "   'center',\n",
       "   'bethesda',\n",
       "   'maryland',\n",
       "   'sponsor',\n",
       "   'ness',\n",
       "   'navy',\n",
       "   'engineering',\n",
       "   'software',\n",
       "   'system',\n",
       "   'sponsoring',\n",
       "   'oneday',\n",
       "   'navy',\n",
       "   'scientific',\n",
       "   'visualization',\n",
       "   'virtual',\n",
       "   'reality',\n",
       "   'seminar',\n",
       "   'purpose',\n",
       "   'seminar',\n",
       "   'present',\n",
       "   'exchange',\n",
       "   'information',\n",
       "   'navyrelated',\n",
       "   'scientific',\n",
       "   'visualization',\n",
       "   'virtual',\n",
       "   'reality',\n",
       "   'programs',\n",
       "   'research',\n",
       "   'developments',\n",
       "   'applications',\n",
       "   'presentations',\n",
       "   'presentations',\n",
       "   'solicited',\n",
       "   'aspects',\n",
       "   'navyrelated',\n",
       "   'scientific',\n",
       "   'visualization',\n",
       "   'virtual',\n",
       "   'reality',\n",
       "   'current',\n",
       "   'work',\n",
       "   'worksinprogress',\n",
       "   'proposed',\n",
       "   'work',\n",
       "   'navy',\n",
       "   'organizations',\n",
       "   'considered',\n",
       "   'four',\n",
       "   'types',\n",
       "   'presentations',\n",
       "   'available',\n",
       "   '1',\n",
       "   'regular',\n",
       "   'presentation',\n",
       "   '2030',\n",
       "   'minutes',\n",
       "   'length',\n",
       "   '2',\n",
       "   'short',\n",
       "   'presentation',\n",
       "   '10',\n",
       "   'minutes',\n",
       "   'length',\n",
       "   '3',\n",
       "   'video',\n",
       "   'presentation',\n",
       "   'standalone',\n",
       "   'videotape',\n",
       "   'author',\n",
       "   'need',\n",
       "   'attend',\n",
       "   'seminar',\n",
       "   '4',\n",
       "   'scientific',\n",
       "   'visualization',\n",
       "   'virtual',\n",
       "   'reality',\n",
       "   'demonstration',\n",
       "   'byoh',\n",
       "   'accepted',\n",
       "   'presentations',\n",
       "   'published',\n",
       "   'proceedings',\n",
       "   'however',\n",
       "   'viewgraphs',\n",
       "   'materials',\n",
       "   'reproduced',\n",
       "   'seminar',\n",
       "   'attendees',\n",
       "   'abstracts',\n",
       "   'authors',\n",
       "   'submit',\n",
       "   'one',\n",
       "   'page',\n",
       "   'abstract',\n",
       "   'andor',\n",
       "   'videotape',\n",
       "   'robert',\n",
       "   'lipman',\n",
       "   'naval',\n",
       "   'surface',\n",
       "   'warfare',\n",
       "   'center',\n",
       "   'carderock',\n",
       "   'division',\n",
       "   'code',\n",
       "   '2042',\n",
       "   'bethesda',\n",
       "   'maryland',\n",
       "   '200845000',\n",
       "   'voice',\n",
       "   '301',\n",
       "   '2273618',\n",
       "   'fax',\n",
       "   '301',\n",
       "   '2275753',\n",
       "   'email',\n",
       "   'lipmanoasysdtnavymil',\n",
       "   'authors',\n",
       "   'include',\n",
       "   'type',\n",
       "   'presentation',\n",
       "   'affiliations',\n",
       "   'addresses',\n",
       "   'telephone',\n",
       "   'fax',\n",
       "   'numbers',\n",
       "   'addresses',\n",
       "   'multiauthor',\n",
       "   'papers',\n",
       "   'designate',\n",
       "   'one',\n",
       "   'point',\n",
       "   'contact',\n",
       "   'deadlines',\n",
       "   'abstact',\n",
       "   'submission',\n",
       "   'deadline',\n",
       "   'april',\n",
       "   '30',\n",
       "   '1993',\n",
       "   'notification',\n",
       "   'acceptance',\n",
       "   'sent',\n",
       "   'may',\n",
       "   '14',\n",
       "   '1993',\n",
       "   'materials',\n",
       "   'reproduction',\n",
       "   'must',\n",
       "   'received',\n",
       "   'june',\n",
       "   '1',\n",
       "   '1993',\n",
       "   'information',\n",
       "   'contact',\n",
       "   'robert',\n",
       "   'lipman',\n",
       "   'address',\n",
       "   'please',\n",
       "   'distribute',\n",
       "   'widely',\n",
       "   'possible',\n",
       "   'thanks',\n",
       "   'robert',\n",
       "   'lipman',\n",
       "   'internet',\n",
       "   'lipmanoasysdtnavymil',\n",
       "   'david',\n",
       "   'taylor',\n",
       "   'model',\n",
       "   'basin',\n",
       "   'cdnswc',\n",
       "   'lipoceandtnavymil',\n",
       "   'computational',\n",
       "   'signatures',\n",
       "   'voicenet',\n",
       "   '301',\n",
       "   '2273618',\n",
       "   'structures',\n",
       "   'group',\n",
       "   'code',\n",
       "   '2042',\n",
       "   'factsnet',\n",
       "   '301',\n",
       "   '2275753',\n",
       "   'bethesda',\n",
       "   'maryland',\n",
       "   '200845000',\n",
       "   'phishnet',\n",
       "   'stockingslonglegs',\n",
       "   'sixth',\n",
       "   'sick',\n",
       "   'shieks',\n",
       "   'sixth',\n",
       "   'sheeps',\n",
       "   'sick']),\n",
       " (1,\n",
       "  ['article',\n",
       "   '1993mar301134367339worakkaistackr',\n",
       "   'tjyuevekaistackr',\n",
       "   'yu',\n",
       "   'taijung',\n",
       "   'writes',\n",
       "   'anybody',\n",
       "   'document',\n",
       "   'rtf',\n",
       "   'file',\n",
       "   'know',\n",
       "   'get',\n",
       "   'thanks',\n",
       "   'advance',\n",
       "   'got',\n",
       "   'one',\n",
       "   'microsoft',\n",
       "   'tech',\n",
       "   'support',\n",
       "   'sterling',\n",
       "   'g',\n",
       "   'bjorndahl',\n",
       "   'bjorndahlaugustanaabca',\n",
       "   'bjorndahlcamroseuucp',\n",
       "   'augustana',\n",
       "   'university',\n",
       "   'college',\n",
       "   'camrose',\n",
       "   'alberta',\n",
       "   'canada',\n",
       "   '403',\n",
       "   '6791100',\n",
       "   'newsgroup',\n",
       "   'composmswindowsmisc',\n",
       "   'documentid',\n",
       "   '9137',\n",
       "   'subject',\n",
       "   'winbench',\n",
       "   '311',\n",
       "   'help',\n",
       "   'graphics',\n",
       "   'comparison',\n",
       "   'srg3sirgrvgracecrinz',\n",
       "   'article',\n",
       "   'c4zogdc56newsudeledu',\n",
       "   'swyattbachudeledu',\n",
       "   'stephen',\n",
       "   'l',\n",
       "   'wyatt',\n",
       "   'writes',\n",
       "   'question',\n",
       "   'winbench',\n",
       "   'pc',\n",
       "   'labs',\n",
       "   'thing',\n",
       "   '311',\n",
       "   '38633',\n",
       "   'ahead',\n",
       "   'b',\n",
       "   '512k',\n",
       "   'card',\n",
       "   'got',\n",
       "   'results',\n",
       "   'windows',\n",
       "   'vga',\n",
       "   'driver',\n",
       "   '244',\n",
       "   'million',\n",
       "   'ahead',\n",
       "   'b',\n",
       "   '640480256',\n",
       "   'driver',\n",
       "   '455000',\n",
       "   'winmarks',\n",
       "   'windows',\n",
       "   'svga',\n",
       "   '80060016',\n",
       "   'driver',\n",
       "   '168',\n",
       "   'million',\n",
       "   'winmarks',\n",
       "   'thinking',\n",
       "   'upgrading',\n",
       "   'diamond',\n",
       "   '24x',\n",
       "   'card',\n",
       "   'read',\n",
       "   '8',\n",
       "   'million',\n",
       "   'winmark',\n",
       "   'obviously',\n",
       "   'much',\n",
       "   'much',\n",
       "   'greater',\n",
       "   '256color',\n",
       "   'mode',\n",
       "   'good',\n",
       "   'cardstreet',\n",
       "   'price',\n",
       "   '170',\n",
       "   '386dx',\n",
       "   '33mhz',\n",
       "   '4mb',\n",
       "   'ram',\n",
       "   'winbench',\n",
       "   '25',\n",
       "   '24x',\n",
       "   'v202',\n",
       "   '167m',\n",
       "   '1668274',\n",
       "   'v203',\n",
       "   '167m',\n",
       "   '1668985',\n",
       "   'v203',\n",
       "   '16',\n",
       "   '4602428',\n",
       "   'v203',\n",
       "   '256',\n",
       "   '7635278',\n",
       "   'richard',\n",
       "   'mayston',\n",
       "   'maystonrgracecrinz']),\n",
       " (2,\n",
       "  ['geoffrey',\n",
       "   'elbo',\n",
       "   'writes',\n",
       "   'yes',\n",
       "   'fastest',\n",
       "   'defrag',\n",
       "   'ive',\n",
       "   'ever',\n",
       "   'watched',\n",
       "   '170mb',\n",
       "   'hard',\n",
       "   'disk',\n",
       "   '20',\n",
       "   'minutes',\n",
       "   'found',\n",
       "   'ms',\n",
       "   'defrag',\n",
       "   'looks',\n",
       "   'much',\n",
       "   'like',\n",
       "   'norton',\n",
       "   'speedisk',\n",
       "   'stripdown',\n",
       "   'version',\n",
       "   'later',\n",
       "   'norton',\n",
       "   'speedisk',\n",
       "   'backup',\n",
       "   'wondering',\n",
       "   'need',\n",
       "   'install',\n",
       "   'ms',\n",
       "   'backup',\n",
       "   'richard']),\n",
       " (3,\n",
       "  ['theres',\n",
       "   'one',\n",
       "   'thing',\n",
       "   'windows',\n",
       "   'really',\n",
       "   'frosts',\n",
       "   '20mb',\n",
       "   'ram',\n",
       "   'installed',\n",
       "   'system',\n",
       "   'use',\n",
       "   '5mb',\n",
       "   '25mb',\n",
       "   'windows',\n",
       "   'diskcache',\n",
       "   '4mb',\n",
       "   'permanent',\n",
       "   'swap',\n",
       "   'file',\n",
       "   'never',\n",
       "   'fill',\n",
       "   'memory',\n",
       "   'still',\n",
       "   'problems',\n",
       "   'sometimes',\n",
       "   'run',\n",
       "   'gdi',\n",
       "   'resources',\n",
       "   'gives',\n",
       "   'think',\n",
       "   'windows',\n",
       "   'could',\n",
       "   'manage',\n",
       "   'resources',\n",
       "   'little',\n",
       "   'better',\n",
       "   'using',\n",
       "   'windows',\n",
       "   '30',\n",
       "   '31',\n",
       "   'youre',\n",
       "   'still',\n",
       "   '30',\n",
       "   '31',\n",
       "   'devotes',\n",
       "   'twice',\n",
       "   'much',\n",
       "   'memory',\n",
       "   'runs',\n",
       "   'much',\n",
       "   'less',\n",
       "   'frequently',\n",
       "   '31',\n",
       "   'might',\n",
       "   'use',\n",
       "   'one',\n",
       "   'resource',\n",
       "   'monitors',\n",
       "   'one',\n",
       "   'comes',\n",
       "   'windows',\n",
       "   '31',\n",
       "   'resource',\n",
       "   'kit',\n",
       "   'one',\n",
       "   'many',\n",
       "   'shareware',\n",
       "   'ones',\n",
       "   'available',\n",
       "   'see',\n",
       "   'programs',\n",
       "   'hogging',\n",
       "   'resources',\n",
       "   'every',\n",
       "   'icon',\n",
       "   'internal',\n",
       "   'graphics',\n",
       "   'brush',\n",
       "   'etc',\n",
       "   'every',\n",
       "   'program',\n",
       "   'running',\n",
       "   'uses',\n",
       "   'certain',\n",
       "   'amount',\n",
       "   'limited',\n",
       "   'memory',\n",
       "   'area',\n",
       "   'also',\n",
       "   'dont',\n",
       "   'give',\n",
       "   'back',\n",
       "   'theyre',\n",
       "   'finished',\n",
       "   'lamont',\n",
       "   'downs',\n",
       "   'downsnevadaedu']),\n",
       " (4,\n",
       "  ['jorgeerexeastsuncom',\n",
       "   'jorge',\n",
       "   'lach',\n",
       "   'sun',\n",
       "   'bos',\n",
       "   'hardware',\n",
       "   'writes',\n",
       "   'ftp',\n",
       "   'site',\n",
       "   'carries',\n",
       "   'winbench',\n",
       "   'results',\n",
       "   'different',\n",
       "   'graphics',\n",
       "   'cards',\n",
       "   'excel',\n",
       "   'xls',\n",
       "   'format',\n",
       "   'latest',\n",
       "   'version',\n",
       "   'winbench',\n",
       "   'differ',\n",
       "   'source',\n",
       "   'available',\n",
       "   'anybody',\n",
       "   'try',\n",
       "   'port',\n",
       "   'xwindow',\n",
       "   'least',\n",
       "   'way',\n",
       "   'make',\n",
       "   'comparisons',\n",
       "   'possible',\n",
       "   'ftpcicaindianaedu',\n",
       "   'pubpcwin3miscwinadvzip',\n",
       "   'writeup',\n",
       "   'steve',\n",
       "   'gibson',\n",
       "   'infoworld',\n",
       "   'winbench',\n",
       "   '311',\n",
       "   'number',\n",
       "   'benchmark',\n",
       "   'results',\n",
       "   'nine',\n",
       "   'isa',\n",
       "   'four',\n",
       "   'vlb',\n",
       "   'video',\n",
       "   'cards',\n",
       "   'current',\n",
       "   'upload',\n",
       "   'likely',\n",
       "   'card',\n",
       "   'youre',\n",
       "   'currently',\n",
       "   'giving',\n",
       "   'serious',\n",
       "   'consideration',\n",
       "   'xls',\n",
       "   'format',\n",
       "   'latest',\n",
       "   'version',\n",
       "   'winbench',\n",
       "   'know',\n",
       "   'ver',\n",
       "   '311',\n",
       "   'believe',\n",
       "   'try',\n",
       "   'maintain',\n",
       "   'rating',\n",
       "   'scale',\n",
       "   'versions',\n",
       "   'new',\n",
       "   'versions',\n",
       "   'released',\n",
       "   'defeat',\n",
       "   'lastest',\n",
       "   'coding',\n",
       "   'tricks',\n",
       "   'put',\n",
       "   'driver',\n",
       "   'programmers',\n",
       "   'beat',\n",
       "   'benchmarks',\n",
       "   'dont',\n",
       "   'know',\n",
       "   'last',\n",
       "   'one',\n",
       "   'al',\n",
       "   'devilbiss',\n",
       "   'alcolhpcom'])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_rdd = df.select(df[\"doc_id\"], tokenize_udf(df[\"Text\"]).alias(\"tokens\")) \\\n",
    "    .rdd.map(lambda x: (x[0], x[1]))\n",
    "tokenized_rdd.cache()  # Cache RDD to improve performance\n",
    "tokenized_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQFxXJ5GaJrf"
   },
   "source": [
    "# 3. TF/IDF Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "hI44aYdBZJuA"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "8gNfe-fyaf_g"
   },
   "outputs": [],
   "source": [
    "bag_of_words = tokenized_rdd.mapValues(lambda x: (Counter(x), len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_dGTAEuchnl",
    "outputId": "39294fe2-12e7-4d92-d118-b892116441ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- token: string (nullable = true)\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- local_count: long (nullable = true)\n",
      " |-- tf: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 15:46:21 WARN BlockManager: Task 624 already completed, not releasing lock for rdd_116_0\n"
     ]
    }
   ],
   "source": [
    "global_tf = bag_of_words.flatMap(lambda x: [(word, x[0], local_count, local_count / x[1][1]) for word, local_count in x[1][0].items()])\n",
    "tf_df = global_tf.toDF([\"token\", \"doc_id\", \"local_count\", \"tf\"])\n",
    "tf_df.cache()\n",
    "tf_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "2iRG4t20dgfS"
   },
   "outputs": [],
   "source": [
    "token_global_df = tf_df.groupBy(\"token\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "TMp3rZ31nFeo"
   },
   "outputs": [],
   "source": [
    "idf = udf(lambda x: math.log(N / x), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SdHODWbgMnw",
    "outputId": "66e3d212-4287-44eb-ad27-168dba75667b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[token: string, idf: double]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_df = token_global_df.withColumn(\"idf\", idf(token_global_df[\"count\"])).select(\"token\", \"idf\")\n",
    "idf_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "o3-ALdRypRdC"
   },
   "outputs": [],
   "source": [
    "tf_idf = udf(lambda tf, idf: tf * idf, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBbZOc87o8hv",
    "outputId": "84e968c1-165c-4cd5-aff9-ad661986a4d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[token: string, doc_id: bigint, local_count: bigint, tf_idf: double]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_df = tf_df.join(idf_df, \"token\")\n",
    "main_df = tf_idf_df.withColumn(\"tf_idf\", tf_idf(tf_idf_df[\"tf\"], tf_idf_df[\"idf\"])).select(\"token\", \"doc_id\", \"local_count\", \"tf_idf\")\n",
    "main_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdv7TdwOnw1-"
   },
   "source": [
    "# (For Testing) Materialization Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EbtjnptFn0IY",
    "outputId": "61aec691-d494-4275-e6ae-0ac591877620"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:============================================>        (168 + 11) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----------+--------------------+\n",
      "|    token|doc_id|local_count|              tf_idf|\n",
      "+---------+------+-----------+--------------------+\n",
      "|    still|     3|          2| 0.04173660549797221|\n",
      "|   waters|     7|          1|0.035255950090013734|\n",
      "|     hope|    10|          1| 0.05026165550283711|\n",
      "|    still|    10|          1|0.035512725730730735|\n",
      "|  blaster|    11|          4|  0.2726847663489077|\n",
      "|readme1st|    11|          1| 0.09812851432276684|\n",
      "|    still|    12|          1|0.016457116802045955|\n",
      "|    input|    12|          1|0.034633267051667214|\n",
      "|     hope|    13|          1|0.010689978968886996|\n",
      "|   online|    20|          4| 0.03928650790629692|\n",
      "|       3x|    20|          2|0.026254808970632214|\n",
      "|    still|    22|          2| 0.08800979855007182|\n",
      "|   online|    29|          1|0.019764011661876833|\n",
      "|     hope|    34|          1| 0.02238214346610715|\n",
      "|    still|    37|          2|0.021196077137713636|\n",
      "|connected|    39|          1| 0.15567358847340163|\n",
      "|qvtnetini|    43|          1|   0.055949105838753|\n",
      "|     hope|    43|          1|0.017576161740255922|\n",
      "|    still|    63|          1|0.017601959710014367|\n",
      "|    still|    65|          1| 0.06325704270786413|\n",
      "+---------+------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# For DataFrames\n",
    "df_to_be_displayed = main_df\n",
    "df_to_be_displayed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7DJFBeMoFAT",
    "outputId": "4b0965df-4ad1-4e2e-8abb-b631f85b6587"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 15:46:29 WARN BlockManager: Task 1230 already completed, not releasing lock for rdd_116_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (Counter({'presentations': 5,\n",
       "            'scientific': 5,\n",
       "            'visualization': 5,\n",
       "            'virtual': 5,\n",
       "            'reality': 5,\n",
       "            'seminar': 5,\n",
       "            'navy': 4,\n",
       "            '1993': 4,\n",
       "            'presentation': 4,\n",
       "            '301': 4,\n",
       "            'center': 3,\n",
       "            'bethesda': 3,\n",
       "            'maryland': 3,\n",
       "            'robert': 3,\n",
       "            'lipman': 3,\n",
       "            'june': 2,\n",
       "            'carderock': 2,\n",
       "            'division': 2,\n",
       "            'naval': 2,\n",
       "            'surface': 2,\n",
       "            'warfare': 2,\n",
       "            'david': 2,\n",
       "            'taylor': 2,\n",
       "            'research': 2,\n",
       "            'information': 2,\n",
       "            'navyrelated': 2,\n",
       "            'work': 2,\n",
       "            '1': 2,\n",
       "            'minutes': 2,\n",
       "            'length': 2,\n",
       "            'videotape': 2,\n",
       "            'materials': 2,\n",
       "            'authors': 2,\n",
       "            'one': 2,\n",
       "            'code': 2,\n",
       "            '2042': 2,\n",
       "            '200845000': 2,\n",
       "            '2273618': 2,\n",
       "            'fax': 2,\n",
       "            '2275753': 2,\n",
       "            'lipmanoasysdtnavymil': 2,\n",
       "            'addresses': 2,\n",
       "            'contact': 2,\n",
       "            'sixth': 2,\n",
       "            'sick': 2,\n",
       "            'call': 1,\n",
       "            'tuesday': 1,\n",
       "            '22': 1,\n",
       "            'formerly': 1,\n",
       "            'sponsor': 1,\n",
       "            'ness': 1,\n",
       "            'engineering': 1,\n",
       "            'software': 1,\n",
       "            'system': 1,\n",
       "            'sponsoring': 1,\n",
       "            'oneday': 1,\n",
       "            'purpose': 1,\n",
       "            'present': 1,\n",
       "            'exchange': 1,\n",
       "            'programs': 1,\n",
       "            'developments': 1,\n",
       "            'applications': 1,\n",
       "            'solicited': 1,\n",
       "            'aspects': 1,\n",
       "            'current': 1,\n",
       "            'worksinprogress': 1,\n",
       "            'proposed': 1,\n",
       "            'organizations': 1,\n",
       "            'considered': 1,\n",
       "            'four': 1,\n",
       "            'types': 1,\n",
       "            'available': 1,\n",
       "            'regular': 1,\n",
       "            '2030': 1,\n",
       "            '2': 1,\n",
       "            'short': 1,\n",
       "            '10': 1,\n",
       "            '3': 1,\n",
       "            'video': 1,\n",
       "            'standalone': 1,\n",
       "            'author': 1,\n",
       "            'need': 1,\n",
       "            'attend': 1,\n",
       "            '4': 1,\n",
       "            'demonstration': 1,\n",
       "            'byoh': 1,\n",
       "            'accepted': 1,\n",
       "            'published': 1,\n",
       "            'proceedings': 1,\n",
       "            'however': 1,\n",
       "            'viewgraphs': 1,\n",
       "            'reproduced': 1,\n",
       "            'attendees': 1,\n",
       "            'abstracts': 1,\n",
       "            'submit': 1,\n",
       "            'page': 1,\n",
       "            'abstract': 1,\n",
       "            'andor': 1,\n",
       "            'voice': 1,\n",
       "            'email': 1,\n",
       "            'include': 1,\n",
       "            'type': 1,\n",
       "            'affiliations': 1,\n",
       "            'telephone': 1,\n",
       "            'numbers': 1,\n",
       "            'multiauthor': 1,\n",
       "            'papers': 1,\n",
       "            'designate': 1,\n",
       "            'point': 1,\n",
       "            'deadlines': 1,\n",
       "            'abstact': 1,\n",
       "            'submission': 1,\n",
       "            'deadline': 1,\n",
       "            'april': 1,\n",
       "            '30': 1,\n",
       "            'notification': 1,\n",
       "            'acceptance': 1,\n",
       "            'sent': 1,\n",
       "            'may': 1,\n",
       "            '14': 1,\n",
       "            'reproduction': 1,\n",
       "            'must': 1,\n",
       "            'received': 1,\n",
       "            'address': 1,\n",
       "            'please': 1,\n",
       "            'distribute': 1,\n",
       "            'widely': 1,\n",
       "            'possible': 1,\n",
       "            'thanks': 1,\n",
       "            'internet': 1,\n",
       "            'model': 1,\n",
       "            'basin': 1,\n",
       "            'cdnswc': 1,\n",
       "            'lipoceandtnavymil': 1,\n",
       "            'computational': 1,\n",
       "            'signatures': 1,\n",
       "            'voicenet': 1,\n",
       "            'structures': 1,\n",
       "            'group': 1,\n",
       "            'factsnet': 1,\n",
       "            'phishnet': 1,\n",
       "            'stockingslonglegs': 1,\n",
       "            'shieks': 1,\n",
       "            'sheeps': 1}),\n",
       "   220)),\n",
       " (1,\n",
       "  (Counter({'winbench': 3,\n",
       "            'driver': 3,\n",
       "            'million': 3,\n",
       "            'v203': 3,\n",
       "            'article': 2,\n",
       "            'writes': 2,\n",
       "            'got': 2,\n",
       "            '311': 2,\n",
       "            'ahead': 2,\n",
       "            'b': 2,\n",
       "            'card': 2,\n",
       "            'windows': 2,\n",
       "            'winmarks': 2,\n",
       "            '24x': 2,\n",
       "            'much': 2,\n",
       "            '167m': 2,\n",
       "            '1993mar301134367339worakkaistackr': 1,\n",
       "            'tjyuevekaistackr': 1,\n",
       "            'yu': 1,\n",
       "            'taijung': 1,\n",
       "            'anybody': 1,\n",
       "            'document': 1,\n",
       "            'rtf': 1,\n",
       "            'file': 1,\n",
       "            'know': 1,\n",
       "            'get': 1,\n",
       "            'thanks': 1,\n",
       "            'advance': 1,\n",
       "            'one': 1,\n",
       "            'microsoft': 1,\n",
       "            'tech': 1,\n",
       "            'support': 1,\n",
       "            'sterling': 1,\n",
       "            'g': 1,\n",
       "            'bjorndahl': 1,\n",
       "            'bjorndahlaugustanaabca': 1,\n",
       "            'bjorndahlcamroseuucp': 1,\n",
       "            'augustana': 1,\n",
       "            'university': 1,\n",
       "            'college': 1,\n",
       "            'camrose': 1,\n",
       "            'alberta': 1,\n",
       "            'canada': 1,\n",
       "            '403': 1,\n",
       "            '6791100': 1,\n",
       "            'newsgroup': 1,\n",
       "            'composmswindowsmisc': 1,\n",
       "            'documentid': 1,\n",
       "            '9137': 1,\n",
       "            'subject': 1,\n",
       "            'help': 1,\n",
       "            'graphics': 1,\n",
       "            'comparison': 1,\n",
       "            'srg3sirgrvgracecrinz': 1,\n",
       "            'c4zogdc56newsudeledu': 1,\n",
       "            'swyattbachudeledu': 1,\n",
       "            'stephen': 1,\n",
       "            'l': 1,\n",
       "            'wyatt': 1,\n",
       "            'question': 1,\n",
       "            'pc': 1,\n",
       "            'labs': 1,\n",
       "            'thing': 1,\n",
       "            '38633': 1,\n",
       "            '512k': 1,\n",
       "            'results': 1,\n",
       "            'vga': 1,\n",
       "            '244': 1,\n",
       "            '640480256': 1,\n",
       "            '455000': 1,\n",
       "            'svga': 1,\n",
       "            '80060016': 1,\n",
       "            '168': 1,\n",
       "            'thinking': 1,\n",
       "            'upgrading': 1,\n",
       "            'diamond': 1,\n",
       "            'read': 1,\n",
       "            '8': 1,\n",
       "            'winmark': 1,\n",
       "            'obviously': 1,\n",
       "            'greater': 1,\n",
       "            '256color': 1,\n",
       "            'mode': 1,\n",
       "            'good': 1,\n",
       "            'cardstreet': 1,\n",
       "            'price': 1,\n",
       "            '170': 1,\n",
       "            '386dx': 1,\n",
       "            '33mhz': 1,\n",
       "            '4mb': 1,\n",
       "            'ram': 1,\n",
       "            '25': 1,\n",
       "            'v202': 1,\n",
       "            '1668274': 1,\n",
       "            '1668985': 1,\n",
       "            '16': 1,\n",
       "            '4602428': 1,\n",
       "            '256': 1,\n",
       "            '7635278': 1,\n",
       "            'richard': 1,\n",
       "            'mayston': 1,\n",
       "            'maystonrgracecrinz': 1}),\n",
       "   122)),\n",
       " (2,\n",
       "  (Counter({'defrag': 2,\n",
       "            'ms': 2,\n",
       "            'norton': 2,\n",
       "            'speedisk': 2,\n",
       "            'backup': 2,\n",
       "            'geoffrey': 1,\n",
       "            'elbo': 1,\n",
       "            'writes': 1,\n",
       "            'yes': 1,\n",
       "            'fastest': 1,\n",
       "            'ive': 1,\n",
       "            'ever': 1,\n",
       "            'watched': 1,\n",
       "            '170mb': 1,\n",
       "            'hard': 1,\n",
       "            'disk': 1,\n",
       "            '20': 1,\n",
       "            'minutes': 1,\n",
       "            'found': 1,\n",
       "            'looks': 1,\n",
       "            'much': 1,\n",
       "            'like': 1,\n",
       "            'stripdown': 1,\n",
       "            'version': 1,\n",
       "            'later': 1,\n",
       "            'wondering': 1,\n",
       "            'need': 1,\n",
       "            'install': 1,\n",
       "            'richard': 1}),\n",
       "   34)),\n",
       " (3,\n",
       "  (Counter({'windows': 5,\n",
       "            'one': 4,\n",
       "            '31': 4,\n",
       "            'memory': 3,\n",
       "            'resources': 3,\n",
       "            'use': 2,\n",
       "            'still': 2,\n",
       "            '30': 2,\n",
       "            'much': 2,\n",
       "            'resource': 2,\n",
       "            'every': 2,\n",
       "            'theres': 1,\n",
       "            'thing': 1,\n",
       "            'really': 1,\n",
       "            'frosts': 1,\n",
       "            '20mb': 1,\n",
       "            'ram': 1,\n",
       "            'installed': 1,\n",
       "            'system': 1,\n",
       "            '5mb': 1,\n",
       "            '25mb': 1,\n",
       "            'diskcache': 1,\n",
       "            '4mb': 1,\n",
       "            'permanent': 1,\n",
       "            'swap': 1,\n",
       "            'file': 1,\n",
       "            'never': 1,\n",
       "            'fill': 1,\n",
       "            'problems': 1,\n",
       "            'sometimes': 1,\n",
       "            'run': 1,\n",
       "            'gdi': 1,\n",
       "            'gives': 1,\n",
       "            'think': 1,\n",
       "            'could': 1,\n",
       "            'manage': 1,\n",
       "            'little': 1,\n",
       "            'better': 1,\n",
       "            'using': 1,\n",
       "            'youre': 1,\n",
       "            'devotes': 1,\n",
       "            'twice': 1,\n",
       "            'runs': 1,\n",
       "            'less': 1,\n",
       "            'frequently': 1,\n",
       "            'might': 1,\n",
       "            'monitors': 1,\n",
       "            'comes': 1,\n",
       "            'kit': 1,\n",
       "            'many': 1,\n",
       "            'shareware': 1,\n",
       "            'ones': 1,\n",
       "            'available': 1,\n",
       "            'see': 1,\n",
       "            'programs': 1,\n",
       "            'hogging': 1,\n",
       "            'icon': 1,\n",
       "            'internal': 1,\n",
       "            'graphics': 1,\n",
       "            'brush': 1,\n",
       "            'etc': 1,\n",
       "            'program': 1,\n",
       "            'running': 1,\n",
       "            'uses': 1,\n",
       "            'certain': 1,\n",
       "            'amount': 1,\n",
       "            'limited': 1,\n",
       "            'area': 1,\n",
       "            'also': 1,\n",
       "            'dont': 1,\n",
       "            'give': 1,\n",
       "            'back': 1,\n",
       "            'theyre': 1,\n",
       "            'finished': 1,\n",
       "            'lamont': 1,\n",
       "            'downs': 1,\n",
       "            'downsnevadaedu': 1}),\n",
       "   97)),\n",
       " (4,\n",
       "  (Counter({'winbench': 4,\n",
       "            'results': 2,\n",
       "            'cards': 2,\n",
       "            'xls': 2,\n",
       "            'format': 2,\n",
       "            'latest': 2,\n",
       "            'version': 2,\n",
       "            'try': 2,\n",
       "            '311': 2,\n",
       "            'know': 2,\n",
       "            'versions': 2,\n",
       "            'jorgeerexeastsuncom': 1,\n",
       "            'jorge': 1,\n",
       "            'lach': 1,\n",
       "            'sun': 1,\n",
       "            'bos': 1,\n",
       "            'hardware': 1,\n",
       "            'writes': 1,\n",
       "            'ftp': 1,\n",
       "            'site': 1,\n",
       "            'carries': 1,\n",
       "            'different': 1,\n",
       "            'graphics': 1,\n",
       "            'excel': 1,\n",
       "            'differ': 1,\n",
       "            'source': 1,\n",
       "            'available': 1,\n",
       "            'anybody': 1,\n",
       "            'port': 1,\n",
       "            'xwindow': 1,\n",
       "            'least': 1,\n",
       "            'way': 1,\n",
       "            'make': 1,\n",
       "            'comparisons': 1,\n",
       "            'possible': 1,\n",
       "            'ftpcicaindianaedu': 1,\n",
       "            'pubpcwin3miscwinadvzip': 1,\n",
       "            'writeup': 1,\n",
       "            'steve': 1,\n",
       "            'gibson': 1,\n",
       "            'infoworld': 1,\n",
       "            'number': 1,\n",
       "            'benchmark': 1,\n",
       "            'nine': 1,\n",
       "            'isa': 1,\n",
       "            'four': 1,\n",
       "            'vlb': 1,\n",
       "            'video': 1,\n",
       "            'current': 1,\n",
       "            'upload': 1,\n",
       "            'likely': 1,\n",
       "            'card': 1,\n",
       "            'youre': 1,\n",
       "            'currently': 1,\n",
       "            'giving': 1,\n",
       "            'serious': 1,\n",
       "            'consideration': 1,\n",
       "            'ver': 1,\n",
       "            'believe': 1,\n",
       "            'maintain': 1,\n",
       "            'rating': 1,\n",
       "            'scale': 1,\n",
       "            'new': 1,\n",
       "            'released': 1,\n",
       "            'defeat': 1,\n",
       "            'lastest': 1,\n",
       "            'coding': 1,\n",
       "            'tricks': 1,\n",
       "            'put': 1,\n",
       "            'driver': 1,\n",
       "            'programmers': 1,\n",
       "            'beat': 1,\n",
       "            'benchmarks': 1,\n",
       "            'dont': 1,\n",
       "            'last': 1,\n",
       "            'one': 1,\n",
       "            'al': 1,\n",
       "            'devilbiss': 1,\n",
       "            'alcolhpcom': 1}),\n",
       "   92))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For RDD\n",
    "rdd_to_be_displayed = bag_of_words\n",
    "rdd_to_be_displayed.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bevbUx-Y9Zau"
   },
   "source": [
    "# 4. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up LDA Tuning Environment ---\n",
      "Checking and downloading NLTK resources if necessary...\n",
      " - Stopwords found.\n",
      " - Downloading WordNet...\n",
      " - WordNet downloaded.\n",
      "NLTK resource check complete.\n",
      "Prerequisite 'spark' available.\n",
      "Prerequisite 'df' available.\n",
      "Prerequisite 'N' available.\n",
      "Prerequisite 'tf_df' available.\n",
      "Tokenization/Lemmatization UDF defined.\n",
      "Applying tokenization and lemmatization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created and cached 'tokenized_rdd' with 18267 documents.\n",
      "Calculating document frequencies for vocabulary filtering...\n",
      "Filtering vocabulary: min_df=5, max_df_ratio=0.85 (max_doc_count=15526)\n",
      "Creating filtered vocabulary maps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Vocabulary Size (V): 25666\n",
      "Filtered vocabulary maps created and broadcasted.\n",
      "Filtering tokens and creating final LDA input RDD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created and cached final 'doc_word_tokens_rdd' with 2280939 filtered tokens.\n",
      "Data preparation (incl. filtering) took 19.69 seconds.\n",
      "\n",
      "--- Starting Hyperparameter Tuning Loop (Varying K) ---\n",
      "\n",
      "===== Processing K = 5 =====\n",
      "Running LDA Gibbs: K=5, alpha=0.100, beta=0.01, iters=100\n",
      "  Initializing random topics...\n",
      "  Calculating initial counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Initial counts collected and broadcasted (n_kv: 116455, n_k: 5, n_d: 18257)\n",
      "  Starting 100 Gibbs sampling iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 1/100 finished in 27.06 seconds. (n_kv size: 106802)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 10/100 finished in 26.00 seconds. (n_kv size: 70928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 20/100 finished in 25.68 seconds. (n_kv size: 58580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 30/100 finished in 26.03 seconds. (n_kv size: 54921)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 40/100 finished in 26.45 seconds. (n_kv size: 53790)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 50/100 finished in 25.69 seconds. (n_kv size: 53543)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 60/100 finished in 25.39 seconds. (n_kv size: 53557)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 70/100 finished in 26.98 seconds. (n_kv size: 53615)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 80/100 finished in 25.89 seconds. (n_kv size: 53797)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 90/100 finished in 27.39 seconds. (n_kv size: 53950)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 100/100 finished in 25.59 seconds. (n_kv size: 54170)\n",
      "  Gibbs loop finished in 2604.27 seconds.\n",
      "  Extracting final Phi distribution...\n",
      "  Cleaning up final LDA RDD and intermediate broadcasts...\n",
      "LDA Gibbs run completed in 2607.56 seconds.\n",
      "--- Calculating Coherence for K=5 ---\n",
      "  Extracting top 10 words...\n",
      "  Calculating document frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Collect word_doc_freq action took: 1.06 seconds\n",
      "  Calculating pair co-document frequencies...\n",
      "    Collect word_pair_cocount action took: 0.38 seconds\n",
      "  Calculating NPMI scores...\n",
      "\n",
      "    Topic 0 Top Words: ['writes', 'one', 'article', 'would', 'like', 'get', 'car', 'dont', 'know', 'time']\n",
      "    Topic 0 Coherence (Avg. NPMI): 0.1335\n",
      "\n",
      "    Topic 1 Top Words: ['people', 'god', 'one', 'would', 'writes', 'article', 'right', 'say', 'government', 'christian']\n",
      "    Topic 1 Coherence (Avg. NPMI): 0.1705\n",
      "\n",
      "    Topic 2 Top Words: ['one', 'would', 'dont', 'think', 'article', 'people', 'writes', 'know', 'like', 'make']\n",
      "    Topic 2 Coherence (Avg. NPMI): 0.1708\n",
      "\n",
      "    Topic 3 Top Words: ['file', 'window', 'use', 'system', 'one', 'program', 'would', 'get', 'drive', 'image']\n",
      "    Topic 3 Coherence (Avg. NPMI): 0.1328\n",
      "\n",
      "    Topic 4 Top Words: ['game', 'year', 'writes', 'team', 'would', 'article', 'one', 'player', 'space', 'first']\n",
      "    Topic 4 Coherence (Avg. NPMI): 0.1331\n",
      "\n",
      "  Overall Average Coherence for K=5: 0.1482\n",
      "--- Coherence calculation finished in 1.52 seconds ---\n",
      "===== Finished processing K = 5 in 2609.09 seconds =====\n",
      "\n",
      "===== Processing K = 15 =====\n",
      "Running LDA Gibbs: K=15, alpha=0.100, beta=0.01, iters=100\n",
      "  Initializing random topics...\n",
      "  Calculating initial counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Initial counts collected and broadcasted (n_kv: 256652, n_k: 15, n_d: 18257)\n",
      "  Starting 100 Gibbs sampling iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 1/100 finished in 34.19 seconds. (n_kv size: 217629)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 10/100 finished in 32.00 seconds. (n_kv size: 121309)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 20/100 finished in 31.39 seconds. (n_kv size: 94626)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 30/100 finished in 30.94 seconds. (n_kv size: 86379)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 40/100 finished in 30.70 seconds. (n_kv size: 83265)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 50/100 finished in 30.18 seconds. (n_kv size: 82163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 60/100 finished in 32.50 seconds. (n_kv size: 81704)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 70/100 finished in 31.27 seconds. (n_kv size: 81629)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 80/100 finished in 33.35 seconds. (n_kv size: 81612)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 90/100 finished in 31.94 seconds. (n_kv size: 81756)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 100/100 finished in 32.44 seconds. (n_kv size: 81879)\n",
      "  Gibbs loop finished in 3168.78 seconds.\n",
      "  Extracting final Phi distribution...\n",
      "  Cleaning up final LDA RDD and intermediate broadcasts...\n",
      "LDA Gibbs run completed in 3172.54 seconds.\n",
      "--- Calculating Coherence for K=15 ---\n",
      "  Extracting top 10 words...\n",
      "  Calculating document frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Collect word_doc_freq action took: 1.06 seconds\n",
      "  Calculating pair co-document frequencies...\n",
      "    Collect word_pair_cocount action took: 0.57 seconds\n",
      "  Calculating NPMI scores...\n",
      "\n",
      "    Topic 0 Top Words: ['game', 'team', 'year', 'player', 'writes', 'would', 'article', 'one', 'last', 'get']\n",
      "    Topic 0 Coherence (Avg. NPMI): 0.1533\n",
      "\n",
      "    Topic 1 Top Words: ['god', 'people', 'would', 'dont', 'think', 'one', 'know', 'say', 'believe', 'writes']\n",
      "    Topic 1 Coherence (Avg. NPMI): 0.1794\n",
      "\n",
      "    Topic 2 Top Words: ['space', 'new', 'earth', 'year', 'system', 'research', 'nasa', 'launch', 'science', 'mission']\n",
      "    Topic 2 Coherence (Avg. NPMI): 0.2247\n",
      "\n",
      "    Topic 3 Top Words: ['car', 'writes', 'article', 'like', 'one', 'get', 'bike', 'would', 'dont', 'dod']\n",
      "    Topic 3 Coherence (Avg. NPMI): 0.1207\n",
      "\n",
      "    Topic 4 Top Words: ['key', 'armenian', 'government', 'chip', 'encryption', 'turkish', 'clipper', 'turk', 'people', 'security']\n",
      "    Topic 4 Coherence (Avg. NPMI): 0.2856\n",
      "\n",
      "    Topic 5 Top Words: ['god', 'jew', 'jesus', 'one', 'church', 'christian', 'jewish', 'christ', 'also', 'would']\n",
      "    Topic 5 Coherence (Avg. NPMI): 0.2784\n",
      "\n",
      "    Topic 6 Top Words: ['window', 'file', 'program', 'problem', 'use', 'do', 'using', 'get', 'driver', 'work']\n",
      "    Topic 6 Coherence (Avg. NPMI): 0.1981\n",
      "\n",
      "    Topic 7 Top Words: ['period', 'university', 'writes', 'shot', 'pt', 'pit', 'goal', 'van', 'bos', 'article']\n",
      "    Topic 7 Coherence (Avg. NPMI): 0.1883\n",
      "\n",
      "    Topic 8 Top Words: ['image', 'file', 'available', 'system', 'software', 'version', 'email', 'program', 'also', 'use']\n",
      "    Topic 8 Coherence (Avg. NPMI): 0.1881\n",
      "\n",
      "    Topic 9 Top Words: ['gun', 'people', 'state', 'right', 'government', 'law', 'president', 'would', 'american', 'article']\n",
      "    Topic 9 Coherence (Avg. NPMI): 0.2136\n",
      "\n",
      "    Topic 10 Top Words: ['drive', 'card', 'one', 'scsi', 'system', 'disk', 'use', 'thanks', 'mac', 'would']\n",
      "    Topic 10 Coherence (Avg. NPMI): 0.1447\n",
      "\n",
      "    Topic 11 Top Words: ['one', 'writes', 'article', 'would', 'say', 'people', 'think', 'dont', 'point', 'question']\n",
      "    Topic 11 Coherence (Avg. NPMI): 0.1766\n",
      "\n",
      "    Topic 12 Top Words: ['one', 'said', 'people', 'know', 'time', 'would', 'say', 'doctor', 'disease', 'medical']\n",
      "    Topic 12 Coherence (Avg. NPMI): 0.1683\n",
      "\n",
      "    Topic 13 Top Words: ['would', 'writes', 'dont', 'article', 'get', 'like', 'one', 'know', 'think', 'thing']\n",
      "    Topic 13 Coherence (Avg. NPMI): 0.1648\n",
      "\n",
      "    Topic 14 Top Words: ['would', 'writes', 'article', 'israel', 'fire', 'israeli', 'arab', 'fbi', 'one', 'people']\n",
      "    Topic 14 Coherence (Avg. NPMI): 0.1672\n",
      "\n",
      "  Overall Average Coherence for K=15: 0.1901\n",
      "--- Coherence calculation finished in 1.72 seconds ---\n",
      "===== Finished processing K = 15 in 3174.29 seconds =====\n",
      "\n",
      "===== Processing K = 25 =====\n",
      "Running LDA Gibbs: K=25, alpha=0.100, beta=0.01, iters=100\n",
      "  Initializing random topics...\n",
      "  Calculating initial counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Initial counts collected and broadcasted (n_kv: 347848, n_k: 25, n_d: 18257)\n",
      "  Starting 100 Gibbs sampling iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 1/100 finished in 42.41 seconds. (n_kv size: 287337)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 10/100 finished in 45.13 seconds. (n_kv size: 150794)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 20/100 finished in 38.70 seconds. (n_kv size: 115105)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 30/100 finished in 42.51 seconds. (n_kv size: 103580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 40/100 finished in 38.69 seconds. (n_kv size: 99114)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 50/100 finished in 37.40 seconds. (n_kv size: 97440)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 60/100 finished in 36.94 seconds. (n_kv size: 96664)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 70/100 finished in 39.10 seconds. (n_kv size: 96558)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 80/100 finished in 38.14 seconds. (n_kv size: 96537)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 90/100 finished in 36.34 seconds. (n_kv size: 96377)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 100/100 finished in 36.56 seconds. (n_kv size: 96404)\n",
      "  Gibbs loop finished in 3856.75 seconds.\n",
      "  Extracting final Phi distribution...\n",
      "  Cleaning up final LDA RDD and intermediate broadcasts...\n",
      "LDA Gibbs run completed in 3860.72 seconds.\n",
      "--- Calculating Coherence for K=25 ---\n",
      "  Extracting top 10 words...\n",
      "  Calculating document frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Collect word_doc_freq action took: 1.06 seconds\n",
      "  Calculating pair co-document frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Collect word_pair_cocount action took: 0.88 seconds\n",
      "  Calculating NPMI scores...\n",
      "\n",
      "    Topic 0 Top Words: ['gun', 'drug', 'use', 'one', 'disease', 'food', 'doctor', 'would', 'patient', 'people']\n",
      "    Topic 0 Coherence (Avg. NPMI): 0.1891\n",
      "\n",
      "    Topic 1 Top Words: ['writes', 'would', 'article', 'fire', 'fbi', 'year', 'koresh', 'one', 'hit', 'batf']\n",
      "    Topic 1 Coherence (Avg. NPMI): 0.1702\n",
      "\n",
      "    Topic 2 Top Words: ['people', 'writes', 'article', 'homosexual', 'sex', 'men', 'would', 'homosexuality', 'child', 'gay']\n",
      "    Topic 2 Coherence (Avg. NPMI): 0.2577\n",
      "\n",
      "    Topic 3 Top Words: ['book', 'one', 'church', 'word', 'time', 'also', 'point', 'text', 'new', 'read']\n",
      "    Topic 3 Coherence (Avg. NPMI): 0.1727\n",
      "\n",
      "    Topic 4 Top Words: ['university', 'san', 'new', 'phone', 'york', 'red', 'fax', 'lost', 'georgia', 'francisco']\n",
      "    Topic 4 Coherence (Avg. NPMI): 0.1719\n",
      "\n",
      "    Topic 5 Top Words: ['drive', 'card', 'system', 'disk', 'scsi', 'problem', 'driver', 'mac', 'work', 'use']\n",
      "    Topic 5 Coherence (Avg. NPMI): 0.2057\n",
      "\n",
      "    Topic 6 Top Words: ['writes', 'article', 'know', 'would', 'dont', 'like', 'anyone', 'post', 'please', 'one']\n",
      "    Topic 6 Coherence (Avg. NPMI): 0.1077\n",
      "\n",
      "    Topic 7 Top Words: ['one', 'say', 'writes', 'evidence', 'would', 'argument', 'claim', 'think', 'moral', 'people']\n",
      "    Topic 7 Coherence (Avg. NPMI): 0.1944\n",
      "\n",
      "    Topic 8 Top Words: ['right', 'israel', 'people', 'israeli', 'state', 'arab', 'jew', 'government', 'law', 'war']\n",
      "    Topic 8 Coherence (Avg. NPMI): 0.3142\n",
      "\n",
      "    Topic 9 Top Words: ['car', 'one', 'would', 'power', 'engine', 'writes', 'use', 'like', 'also', 'article']\n",
      "    Topic 9 Coherence (Avg. NPMI): 0.1198\n",
      "\n",
      "    Topic 10 Top Words: ['window', 'file', 'program', 'use', 'problem', 'application', 'get', 'server', 'using', 'font']\n",
      "    Topic 10 Coherence (Avg. NPMI): 0.2168\n",
      "\n",
      "    Topic 11 Top Words: ['university', 'information', 'april', 'research', 'state', 'center', 'national', 'health', 'may', 'conference']\n",
      "    Topic 11 Coherence (Avg. NPMI): 0.2269\n",
      "\n",
      "    Topic 12 Top Words: ['game', 'team', 'hockey', 'player', 'play', 'win', 'season', 'year', 'goal', 'fan']\n",
      "    Topic 12 Coherence (Avg. NPMI): 0.3810\n",
      "\n",
      "    Topic 13 Top Words: ['writes', 'article', 'year', 'like', 'would', 'number', 'player', 'one', 'good', 'think']\n",
      "    Topic 13 Coherence (Avg. NPMI): 0.1360\n",
      "\n",
      "    Topic 14 Top Words: ['writes', 'bike', 'article', 'dod', 'get', 'like', 'ride', 'car', 'one', 'dog']\n",
      "    Topic 14 Coherence (Avg. NPMI): 0.1502\n",
      "\n",
      "    Topic 15 Top Words: ['president', 'would', 'government', 'think', 'people', 'tax', 'going', 'state', 'know', 'said']\n",
      "    Topic 15 Coherence (Avg. NPMI): 0.1731\n",
      "\n",
      "    Topic 16 Top Words: ['key', 'chip', 'encryption', 'government', 'use', 'clipper', 'system', 'phone', 'would', 'law']\n",
      "    Topic 16 Coherence (Avg. NPMI): 0.2449\n",
      "\n",
      "    Topic 17 Top Words: ['armenian', 'turkish', 'turk', 'people', 'armenia', 'turkey', 'war', 'greek', 'muslim', 'russian']\n",
      "    Topic 17 Coherence (Avg. NPMI): 0.5316\n",
      "\n",
      "    Topic 18 Top Words: ['god', 'christian', 'jesus', 'say', 'one', 'christ', 'bible', 'would', 'people', 'faith']\n",
      "    Topic 18 Coherence (Avg. NPMI): 0.3062\n",
      "\n",
      "    Topic 19 Top Words: ['one', 'said', 'people', 'time', 'went', 'say', 'back', 'didnt', 'know', 'day']\n",
      "    Topic 19 Coherence (Avg. NPMI): 0.1761\n",
      "\n",
      "    Topic 20 Top Words: ['writes', 'article', 'know', 'would', 'dont', 'get', 'like', 'anyone', 'one', 'think']\n",
      "    Topic 20 Coherence (Avg. NPMI): 0.1401\n",
      "\n",
      "    Topic 21 Top Words: ['space', 'earth', 'launch', 'nasa', 'system', 'mission', 'satellite', 'orbit', 'would', 'shuttle']\n",
      "    Topic 21 Coherence (Avg. NPMI): 0.3302\n",
      "\n",
      "    Topic 22 Top Words: ['sale', 'price', 'game', 'new', 'offer', 'email', 'please', 'shipping', 'sell', 'asking']\n",
      "    Topic 22 Coherence (Avg. NPMI): 0.1849\n",
      "\n",
      "    Topic 23 Top Words: ['image', 'file', 'available', 'software', 'program', 'system', 'version', 'graphic', 'also', 'ftp']\n",
      "    Topic 23 Coherence (Avg. NPMI): 0.2389\n",
      "\n",
      "    Topic 24 Top Words: ['dont', 'would', 'like', 'think', 'get', 'people', 'one', 'know', 'thing', 'make']\n",
      "    Topic 24 Coherence (Avg. NPMI): 0.1843\n",
      "\n",
      "  Overall Average Coherence for K=25: 0.2210\n",
      "--- Coherence calculation finished in 2.06 seconds ---\n",
      "===== Finished processing K = 25 in 3862.80 seconds =====\n",
      "\n",
      "===== Processing K = 35 =====\n",
      "Running LDA Gibbs: K=35, alpha=0.100, beta=0.01, iters=100\n",
      "  Initializing random topics...\n",
      "  Calculating initial counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Initial counts collected and broadcasted (n_kv: 418581, n_k: 35, n_d: 18257)\n",
      "  Starting 100 Gibbs sampling iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 1/100 finished in 49.79 seconds. (n_kv size: 340845)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 10/100 finished in 46.43 seconds. (n_kv size: 172231)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 20/100 finished in 43.23 seconds. (n_kv size: 128461)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 30/100 finished in 44.29 seconds. (n_kv size: 115706)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 40/100 finished in 43.85 seconds. (n_kv size: 110928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 50/100 finished in 43.85 seconds. (n_kv size: 108641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 60/100 finished in 43.29 seconds. (n_kv size: 107846)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 70/100 finished in 43.16 seconds. (n_kv size: 107197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 80/100 finished in 42.27 seconds. (n_kv size: 107205)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 90/100 finished in 42.32 seconds. (n_kv size: 107030)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 100/100 finished in 42.51 seconds. (n_kv size: 107144)\n",
      "  Gibbs loop finished in 4440.30 seconds.\n",
      "  Extracting final Phi distribution...\n",
      "  Cleaning up final LDA RDD and intermediate broadcasts...\n",
      "LDA Gibbs run completed in 4444.53 seconds.\n",
      "--- Calculating Coherence for K=35 ---\n",
      "  Extracting top 10 words...\n",
      "  Calculating document frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Collect word_doc_freq action took: 1.03 seconds\n",
      "  Calculating pair co-document frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Collect word_pair_cocount action took: 1.30 seconds\n",
      "  Calculating NPMI scores...\n",
      "\n",
      "    Topic 0 Top Words: ['window', 'do', 'driver', 'file', 'problem', 'use', 'system', 'program', 'work', 'software']\n",
      "    Topic 0 Coherence (Avg. NPMI): 0.2128\n",
      "\n",
      "    Topic 1 Top Words: ['drive', 'card', 'scsi', 'disk', 'system', 'mac', 'hard', 'controller', 'apple', 'bit']\n",
      "    Topic 1 Coherence (Avg. NPMI): 0.2503\n",
      "\n",
      "    Topic 2 Top Words: ['thanks', 'know', 'please', 'anyone', 'would', 'email', 'help', 'advance', 'like', 'looking']\n",
      "    Topic 2 Coherence (Avg. NPMI): 0.1203\n",
      "\n",
      "    Topic 3 Top Words: ['team', 'play', 'player', 'game', 'goal', 'period', 'season', 'shot', 'hockey', 'pit']\n",
      "    Topic 3 Coherence (Avg. NPMI): 0.3457\n",
      "\n",
      "    Topic 4 Top Words: ['file', 'list', 'entry', 'send', 'program', 'address', 'name', 'section', 'line', 'use']\n",
      "    Topic 4 Coherence (Avg. NPMI): 0.2057\n",
      "\n",
      "    Topic 5 Top Words: ['university', 'article', 'writes', 'email', 'please', 'subject', 'internet', 'anyone', 'know', 'thanks']\n",
      "    Topic 5 Coherence (Avg. NPMI): 0.0705\n",
      "\n",
      "    Topic 6 Top Words: ['god', 'one', 'say', 'believe', 'belief', 'people', 'religion', 'christian', 'think', 'dont']\n",
      "    Topic 6 Coherence (Avg. NPMI): 0.2502\n",
      "\n",
      "    Topic 7 Top Words: ['food', 'also', 'would', 'use', 'cause', 'article', 'one', 'problem', 'writes', 'effect']\n",
      "    Topic 7 Coherence (Avg. NPMI): 0.1383\n",
      "\n",
      "    Topic 8 Top Words: ['dont', 'writes', 'like', 'article', 'know', 'think', 'get', 'would', 'thing', 'people']\n",
      "    Topic 8 Coherence (Avg. NPMI): 0.1716\n",
      "\n",
      "    Topic 9 Top Words: ['one', 'theory', 'point', 'two', 'bit', 'time', 'number', 'energy', 'block', 'used']\n",
      "    Topic 9 Coherence (Avg. NPMI): 0.1574\n",
      "\n",
      "    Topic 10 Top Words: ['car', 'engine', 'price', 'new', 'sale', 'model', 'mile', 'also', 'one', 'offer']\n",
      "    Topic 10 Coherence (Avg. NPMI): 0.1519\n",
      "\n",
      "    Topic 11 Top Words: ['israel', 'israeli', 'arab', 'writes', 'article', 'palestinian', 'would', 'people', 'right', 'civilian']\n",
      "    Topic 11 Coherence (Avg. NPMI): 0.2541\n",
      "\n",
      "    Topic 12 Top Words: ['one', 'like', 'writes', 'would', 'power', 'use', 'ive', 'sound', 'get', 'line']\n",
      "    Topic 12 Coherence (Avg. NPMI): 0.1142\n",
      "\n",
      "    Topic 13 Top Words: ['dont', 'would', 'think', 'people', 'know', 'get', 'like', 'one', 'make', 'want']\n",
      "    Topic 13 Coherence (Avg. NPMI): 0.1771\n",
      "\n",
      "    Topic 14 Top Words: ['homosexual', 'people', 'writes', 'article', 'men', 'would', 'sex', 'child', 'homosexuality', 'gay']\n",
      "    Topic 14 Coherence (Avg. NPMI): 0.2577\n",
      "\n",
      "    Topic 15 Top Words: ['writes', 'article', 'would', 'dont', 'water', 'one', 'think', 'get', 'know', 'company']\n",
      "    Topic 15 Coherence (Avg. NPMI): 0.1284\n",
      "\n",
      "    Topic 16 Top Words: ['would', 'one', 'question', 'may', 'could', 'make', 'people', 'even', 'way', 'time']\n",
      "    Topic 16 Coherence (Avg. NPMI): 0.1892\n",
      "\n",
      "    Topic 17 Top Words: ['key', 'chip', 'encryption', 'government', 'clipper', 'phone', 'system', 'use', 'security', 'privacy']\n",
      "    Topic 17 Coherence (Avg. NPMI): 0.3301\n",
      "\n",
      "    Topic 18 Top Words: ['god', 'jesus', 'christian', 'church', 'christ', 'one', 'bible', 'sin', 'lord', 'say']\n",
      "    Topic 18 Coherence (Avg. NPMI): 0.3914\n",
      "\n",
      "    Topic 19 Top Words: ['fire', 'would', 'fbi', 'writes', 'article', 'koresh', 'batf', 'child', 'people', 'government']\n",
      "    Topic 19 Coherence (Avg. NPMI): 0.2447\n",
      "\n",
      "    Topic 20 Top Words: ['drug', 'insurance', 'wire', 'use', 'ground', 'canada', 'one', 'article', 'get', 'writes']\n",
      "    Topic 20 Coherence (Avg. NPMI): 0.1011\n",
      "\n",
      "    Topic 21 Top Words: ['game', 'year', 'team', 'writes', 'player', 'last', 'article', 'fan', 'baseball', 'run']\n",
      "    Topic 21 Coherence (Avg. NPMI): 0.2127\n",
      "\n",
      "    Topic 22 Top Words: ['earth', 'light', 'would', 'planet', 'detector', 'moon', 'one', 'orbit', 'radar', 'sky']\n",
      "    Topic 22 Coherence (Avg. NPMI): 0.2184\n",
      "\n",
      "    Topic 23 Top Words: ['book', 'new', 'list', 'copy', 'art', 'internet', 'cover', 'appears', 'issue', 'email']\n",
      "    Topic 23 Coherence (Avg. NPMI): 0.1700\n",
      "\n",
      "    Topic 24 Top Words: ['space', 'system', 'nasa', 'launch', 'mission', 'research', 'data', 'technology', 'science', 'shuttle']\n",
      "    Topic 24 Coherence (Avg. NPMI): 0.2928\n",
      "\n",
      "    Topic 25 Top Words: ['image', 'file', 'format', 'available', 'program', 'ftp', 'software', 'version', 'graphic', 'color']\n",
      "    Topic 25 Coherence (Avg. NPMI): 0.2885\n",
      "\n",
      "    Topic 26 Top Words: ['gun', 'right', 'law', 'state', 'weapon', 'government', 'people', 'firearm', 'court', 'crime']\n",
      "    Topic 26 Coherence (Avg. NPMI): 0.3132\n",
      "\n",
      "    Topic 27 Top Words: ['armenian', 'turkish', 'turk', 'armenia', 'greek', 'turkey', 'russian', 'people', 'genocide', 'azerbaijan']\n",
      "    Topic 27 Coherence (Avg. NPMI): 0.5810\n",
      "\n",
      "    Topic 28 Top Words: ['said', 'one', 'time', 'people', 'went', 'didnt', 'back', 'say', 'know', 'day']\n",
      "    Topic 28 Coherence (Avg. NPMI): 0.1761\n",
      "\n",
      "    Topic 29 Top Words: ['president', 'state', 'medical', 'health', 'disease', 'stephanopoulos', 'year', 'said', 'new', 'april']\n",
      "    Topic 29 Coherence (Avg. NPMI): 0.2132\n",
      "\n",
      "    Topic 30 Top Words: ['jew', 'jewish', 'state', 'war', 'country', 'right', 'world', 'nazi', 'people', 'group']\n",
      "    Topic 30 Coherence (Avg. NPMI): 0.2664\n",
      "\n",
      "    Topic 31 Top Words: ['bike', 'writes', 'article', 'dod', 'like', 'ride', 'get', 'dog', 'motorcycle', 'one']\n",
      "    Topic 31 Coherence (Avg. NPMI): 0.1741\n",
      "\n",
      "    Topic 32 Top Words: ['writes', 'article', 'one', 'book', 'would', 'evidence', 'claim', 'think', 'read', 'people']\n",
      "    Topic 32 Coherence (Avg. NPMI): 0.1670\n",
      "\n",
      "    Topic 33 Top Words: ['window', 'server', 'application', 'use', 'widget', 'using', 'sun', 'motif', 'program', 'get']\n",
      "    Topic 33 Coherence (Avg. NPMI): 0.2313\n",
      "\n",
      "    Topic 34 Top Words: ['muslim', 'writes', 'article', 'war', 'world', 'serb', 'one', 'german', 'like', 'bosnian']\n",
      "    Topic 34 Coherence (Avg. NPMI): 0.2099\n",
      "\n",
      "  Overall Average Coherence for K=35: 0.2222\n",
      "--- Coherence calculation finished in 2.47 seconds ---\n",
      "===== Finished processing K = 35 in 4447.03 seconds =====\n",
      "\n",
      "--- Tuning Complete ---\n",
      "Total tuning time: 14093.20 seconds\n",
      "\n",
      "K vs. Average Coherence:\n",
      "  K=5: 0.1482\n",
      "  K=15: 0.1901\n",
      "  K=25: 0.2210\n",
      "  K=35: 0.2222\n",
      "\n",
      "Best K found: 35 with coherence 0.2222\n",
      "\n",
      "--- Top 10 Words for Best K=35 ---\n",
      "  Topic 0: ['window', 'do', 'driver', 'file', 'problem', 'use', 'system', 'program', 'work', 'software']\n",
      "  Topic 1: ['drive', 'card', 'scsi', 'disk', 'system', 'mac', 'hard', 'controller', 'apple', 'bit']\n",
      "  Topic 2: ['thanks', 'know', 'please', 'anyone', 'would', 'email', 'help', 'advance', 'like', 'looking']\n",
      "  Topic 3: ['team', 'play', 'player', 'game', 'goal', 'period', 'season', 'shot', 'hockey', 'pit']\n",
      "  Topic 4: ['file', 'list', 'entry', 'send', 'program', 'address', 'name', 'section', 'line', 'use']\n",
      "  Topic 5: ['university', 'article', 'writes', 'email', 'please', 'subject', 'internet', 'anyone', 'know', 'thanks']\n",
      "  Topic 6: ['god', 'one', 'say', 'believe', 'belief', 'people', 'religion', 'christian', 'think', 'dont']\n",
      "  Topic 7: ['food', 'also', 'would', 'use', 'cause', 'article', 'one', 'problem', 'writes', 'effect']\n",
      "  Topic 8: ['dont', 'writes', 'like', 'article', 'know', 'think', 'get', 'would', 'thing', 'people']\n",
      "  Topic 9: ['one', 'theory', 'point', 'two', 'bit', 'time', 'number', 'energy', 'block', 'used']\n",
      "  Topic 10: ['car', 'engine', 'price', 'new', 'sale', 'model', 'mile', 'also', 'one', 'offer']\n",
      "  Topic 11: ['israel', 'israeli', 'arab', 'writes', 'article', 'palestinian', 'would', 'people', 'right', 'civilian']\n",
      "  Topic 12: ['one', 'like', 'writes', 'would', 'power', 'use', 'ive', 'sound', 'get', 'line']\n",
      "  Topic 13: ['dont', 'would', 'think', 'people', 'know', 'get', 'like', 'one', 'make', 'want']\n",
      "  Topic 14: ['homosexual', 'people', 'writes', 'article', 'men', 'would', 'sex', 'child', 'homosexuality', 'gay']\n",
      "  Topic 15: ['writes', 'article', 'would', 'dont', 'water', 'one', 'think', 'get', 'know', 'company']\n",
      "  Topic 16: ['would', 'one', 'question', 'may', 'could', 'make', 'people', 'even', 'way', 'time']\n",
      "  Topic 17: ['key', 'chip', 'encryption', 'government', 'clipper', 'phone', 'system', 'use', 'security', 'privacy']\n",
      "  Topic 18: ['god', 'jesus', 'christian', 'church', 'christ', 'one', 'bible', 'sin', 'lord', 'say']\n",
      "  Topic 19: ['fire', 'would', 'fbi', 'writes', 'article', 'koresh', 'batf', 'child', 'people', 'government']\n",
      "  Topic 20: ['drug', 'insurance', 'wire', 'use', 'ground', 'canada', 'one', 'article', 'get', 'writes']\n",
      "  Topic 21: ['game', 'year', 'team', 'writes', 'player', 'last', 'article', 'fan', 'baseball', 'run']\n",
      "  Topic 22: ['earth', 'light', 'would', 'planet', 'detector', 'moon', 'one', 'orbit', 'radar', 'sky']\n",
      "  Topic 23: ['book', 'new', 'list', 'copy', 'art', 'internet', 'cover', 'appears', 'issue', 'email']\n",
      "  Topic 24: ['space', 'system', 'nasa', 'launch', 'mission', 'research', 'data', 'technology', 'science', 'shuttle']\n",
      "  Topic 25: ['image', 'file', 'format', 'available', 'program', 'ftp', 'software', 'version', 'graphic', 'color']\n",
      "  Topic 26: ['gun', 'right', 'law', 'state', 'weapon', 'government', 'people', 'firearm', 'court', 'crime']\n",
      "  Topic 27: ['armenian', 'turkish', 'turk', 'armenia', 'greek', 'turkey', 'russian', 'people', 'genocide', 'azerbaijan']\n",
      "  Topic 28: ['said', 'one', 'time', 'people', 'went', 'didnt', 'back', 'say', 'know', 'day']\n",
      "  Topic 29: ['president', 'state', 'medical', 'health', 'disease', 'stephanopoulos', 'year', 'said', 'new', 'april']\n",
      "  Topic 30: ['jew', 'jewish', 'state', 'war', 'country', 'right', 'world', 'nazi', 'people', 'group']\n",
      "  Topic 31: ['bike', 'writes', 'article', 'dod', 'like', 'ride', 'get', 'dog', 'motorcycle', 'one']\n",
      "  Topic 32: ['writes', 'article', 'one', 'book', 'would', 'evidence', 'claim', 'think', 'read', 'people']\n",
      "  Topic 33: ['window', 'server', 'application', 'use', 'widget', 'using', 'sun', 'motif', 'program', 'get']\n",
      "  Topic 34: ['muslim', 'writes', 'article', 'war', 'world', 'serb', 'one', 'german', 'like', 'bosnian']\n",
      "--- End of Script ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Consolidated LDA Implementation, Tuning (K), and Coherence Evaluation Code\n",
    "# VERSION 5: Fixed prep_start_time definition\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time # Make sure time is imported\n",
    "import math\n",
    "from itertools import combinations\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, collect_list, struct, monotonically_increasing_id, udf\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType, StructType, StructField, StringType\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# --- NLTK Imports and Downloads (Add near top imports) ---\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "print(\"--- Setting up LDA Tuning Environment ---\")\n",
    "\n",
    "# =================================================\n",
    "# Section 0: Prerequisites Check & Setup (REVISED)\n",
    "# =================================================\n",
    "\n",
    "# --- NLTK Downloads ---\n",
    "print(\"Checking and downloading NLTK resources if necessary...\")\n",
    "try:\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    print(\" - Stopwords found.\")\n",
    "except LookupError:\n",
    "    print(\" - Downloading stopwords...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    print(\" - Stopwords downloaded.\")\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    print(\" - WordNet found.\")\n",
    "except LookupError:\n",
    "    print(\" - Downloading WordNet...\")\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    print(\" - WordNet downloaded.\")\n",
    "print(\"NLTK resource check complete.\")\n",
    "\n",
    "# --- Essential variable checks ---\n",
    "# Assumes spark, df, N, tf_df exist from previous cells\n",
    "required_vars = ['spark', 'df', 'N', 'tf_df']\n",
    "prerequisites_ok = True\n",
    "for var_name in required_vars:\n",
    "    if var_name not in globals():\n",
    "        print(f\"ERROR: Prerequisite '{var_name}' is not defined. Please run previous cells.\")\n",
    "        prerequisites_ok = False\n",
    "    else:\n",
    "        print(f\"Prerequisite '{var_name}' available.\")\n",
    "if not prerequisites_ok:\n",
    "    raise NameError(\"Missing prerequisite variables.\")\n",
    "\n",
    "# --- Define REVISED Tokenization/Lemmatization Function ---\n",
    "def tokenize_and_lemmatize(doc):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    if doc is None: return []\n",
    "    try:\n",
    "        text = doc.lower()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text) # Keep only letters and spaces\n",
    "        tokens = re.split(r'\\s+', text)\n",
    "        lemmatized_tokens = [\n",
    "            lemmatizer.lemmatize(w) for w in tokens\n",
    "            if w not in STOPWORDS and len(w) > 2 # Min word length > 2\n",
    "        ]\n",
    "        return lemmatized_tokens\n",
    "    except Exception as e:\n",
    "        # print(f\"Error tokenizing/lemmatizing doc: {e}. Doc: {doc[:50]}...\") # Reduce verbosity\n",
    "        return []\n",
    "\n",
    "tokenize_udf = udf(tokenize_and_lemmatize, ArrayType(StringType()))\n",
    "print(\"Tokenization/Lemmatization UDF defined.\")\n",
    "\n",
    "# --- Apply Tokenization/Lemmatization ---\n",
    "print(\"Applying tokenization and lemmatization...\")\n",
    "# Start timing *before* the first Spark action related to prep\n",
    "prep_start_time = time.time()\n",
    "\n",
    "tokenized_rdd = df.select(\"doc_id\", tokenize_udf(col(\"Text\")).alias(\"tokens\")) \\\n",
    "                  .rdd.map(lambda row: (row.doc_id, row.tokens))\n",
    "tokenized_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "tokenized_count = tokenized_rdd.count() # Action to trigger computation and caching\n",
    "print(f\"Created and cached 'tokenized_rdd' with {tokenized_count} documents.\")\n",
    "\n",
    "# --- Calculate Document Frequencies and Filter Vocabulary ---\n",
    "print(\"Calculating document frequencies for vocabulary filtering...\")\n",
    "doc_unique_words_rdd = tokenized_rdd.mapValues(lambda words: list(set(words))).cache()\n",
    "word_doc_counts_rdd = doc_unique_words_rdd.flatMap(lambda x: [(word, 1) for word in x[1]])\n",
    "word_doc_freq_rdd = word_doc_counts_rdd.reduceByKey(lambda a, b: a + b).cache()\n",
    "\n",
    "min_df = 5\n",
    "max_df_ratio = 0.85\n",
    "max_doc_count = N * max_df_ratio\n",
    "print(f\"Filtering vocabulary: min_df={min_df}, max_df_ratio={max_df_ratio:.2f} (max_doc_count={int(max_doc_count)})\")\n",
    "filtered_word_doc_freq_rdd = word_doc_freq_rdd.filter(\n",
    "    lambda word_count: word_count[1] >= min_df and word_count[1] <= max_doc_count\n",
    ")\n",
    "\n",
    "# --- Create Filtered Vocabulary and Mappings ---\n",
    "print(\"Creating filtered vocabulary maps...\")\n",
    "filtered_vocabulary_rdd = filtered_word_doc_freq_rdd.map(lambda x: x[0]).zipWithIndex()\n",
    "V = filtered_vocabulary_rdd.count()\n",
    "print(f\"Filtered Vocabulary Size (V): {V}\")\n",
    "if V == 0: raise ValueError(\"Vocabulary is empty after filtering!\")\n",
    "\n",
    "vocab_map = filtered_vocabulary_rdd.collectAsMap()\n",
    "vocab_broadcast = spark.sparkContext.broadcast(vocab_map)\n",
    "index_to_word_map = {v: k for k, v in vocab_map.items()}\n",
    "index_to_word_broadcast = spark.sparkContext.broadcast(index_to_word_map)\n",
    "print(\"Filtered vocabulary maps created and broadcasted.\")\n",
    "\n",
    "# --- Filter Original Tokens and Prepare Final Input RDD ---\n",
    "print(\"Filtering tokens and creating final LDA input RDD...\")\n",
    "filtered_tokenized_rdd = tokenized_rdd.mapValues(\n",
    "    lambda tokens: [token for token in tokens if token in vocab_broadcast.value]\n",
    ").cache()\n",
    "\n",
    "def get_filtered_word_id(token):\n",
    "    return vocab_broadcast.value.get(token, -1)\n",
    "get_filtered_word_id_udf = udf(get_filtered_word_id, IntegerType())\n",
    "\n",
    "temp_word_counts_rdd = filtered_tokenized_rdd.flatMap(\n",
    "    lambda x: [( (x[0], token), 1 ) for token in x[1]]\n",
    ").reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "word_counts_df_filtered = temp_word_counts_rdd.map(\n",
    "    lambda x: Row(doc_id=x[0][0], token=x[0][1], local_count=x[1])\n",
    ").toDF().withColumn(\"word_id\", get_filtered_word_id_udf(col(\"token\"))) \\\n",
    "       .select(\"doc_id\", \"word_id\", \"local_count\") \\\n",
    "       .filter(col(\"word_id\") != -1)\n",
    "word_counts_df_filtered.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "doc_word_tokens_rdd = word_counts_df_filtered.rdd.flatMap(\n",
    "    lambda row: [(row.doc_id, row.word_id)] * row.local_count\n",
    ")\n",
    "doc_word_tokens_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "num_tokens = doc_word_tokens_rdd.count()\n",
    "print(f\"Created and cached final 'doc_word_tokens_rdd' with {num_tokens} filtered tokens.\")\n",
    "\n",
    "# End timing for prep section\n",
    "prep_duration = time.time() - prep_start_time\n",
    "print(f\"Data preparation (incl. filtering) took {prep_duration:.2f} seconds.\")\n",
    "\n",
    "# --- Cleanup intermediate RDDs ---\n",
    "word_doc_freq_rdd.unpersist()\n",
    "doc_unique_words_rdd.unpersist()\n",
    "filtered_tokenized_rdd.unpersist() # Optional cleanup\n",
    "word_counts_df_filtered.unpersist()\n",
    "# Keep 'tokenized_rdd' (original lemmatized) cached for coherence\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Section 1: Core LDA Gibbs Sampling Functions\n",
    "# =================================================\n",
    "\n",
    "def sample_partition(partition, K_local, alpha_local, beta_local, V_local,\n",
    "                     n_kv_broadcast_local, n_k_broadcast_local, n_d_broadcast_local):\n",
    "    \"\"\"Performs Gibbs sampling update for a partition of word tokens.\"\"\"\n",
    "    current_n_kv = n_kv_broadcast_local.value\n",
    "    current_n_k = n_k_broadcast_local.value\n",
    "    current_n_d = n_d_broadcast_local.value\n",
    "\n",
    "    local_partition_list = list(partition)\n",
    "    if not local_partition_list: return iter([])\n",
    "\n",
    "    local_n_dk = {}\n",
    "    for doc_id, word_id, topic in local_partition_list:\n",
    "        key = (doc_id, topic)\n",
    "        local_n_dk[key] = local_n_dk.get(key, 0) + 1\n",
    "\n",
    "    results = []\n",
    "    K_alpha_term = K_local * alpha_local\n",
    "    V_beta_term = V_local * beta_local\n",
    "    epsilon = 1e-9\n",
    "\n",
    "    for doc_id, word_id, old_topic in local_partition_list:\n",
    "        local_n_dk[(doc_id, old_topic)] -= 1\n",
    "        nd = current_n_d.get(doc_id, 0) - 1\n",
    "        nd = max(0, nd)\n",
    "\n",
    "        probabilities = np.zeros(K_local)\n",
    "        term1_den = nd + K_alpha_term\n",
    "\n",
    "        for k in range(K_local):\n",
    "            ndk = local_n_dk.get((doc_id, k), 0)\n",
    "            nkv = current_n_kv.get((word_id, k), 0)\n",
    "            nk = current_n_k.get(k, 0)\n",
    "\n",
    "            term1 = (ndk + alpha_local) / term1_den if term1_den > 0 else 0\n",
    "            term2_den = nk + V_beta_term\n",
    "            term2 = (nkv + beta_local) / term2_den if term2_den > 0 else 0\n",
    "            probabilities[k] = term1 * term2\n",
    "\n",
    "        prob_sum = np.sum(probabilities)\n",
    "        if prob_sum <= epsilon:\n",
    "            new_topic = random.randint(0, K_local - 1)\n",
    "        else:\n",
    "            normalized_probs = probabilities / prob_sum\n",
    "            if abs(normalized_probs.sum() - 1.0) > 1e-6 :\n",
    "                 normalized_probs /= normalized_probs.sum()\n",
    "            try:\n",
    "                new_topic = np.random.choice(K_local, p=normalized_probs)\n",
    "            except ValueError as e:\n",
    "                 new_topic = random.randint(0, K_local - 1)\n",
    "\n",
    "        local_n_dk[(doc_id, new_topic)] = local_n_dk.get((doc_id, new_topic), 0) + 1\n",
    "        results.append((doc_id, word_id, new_topic))\n",
    "\n",
    "    return iter(results)\n",
    "\n",
    "\n",
    "def run_lda_gibbs(doc_word_tokens_rdd_local, K_local, alpha_local, beta_local, iterations_local, V_local, N_local, spark_context):\n",
    "    \"\"\"Runs the complete LDA Gibbs sampling process.\"\"\"\n",
    "    print(f\"Running LDA Gibbs: K={K_local}, alpha={alpha_local:.3f}, beta={beta_local}, iters={iterations_local}\")\n",
    "    lda_internal_start_time = time.time()\n",
    "\n",
    "    print(\"  Initializing random topics...\")\n",
    "    doc_word_topic_rdd = doc_word_tokens_rdd_local.map(\n",
    "        lambda x: (x[0], x[1], random.randint(0, K_local - 1))\n",
    "    ).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    print(\"  Calculating initial counts...\")\n",
    "    n_kv_rdd = doc_word_topic_rdd.map(lambda x: ((x[1], x[2]), 1)).reduceByKey(lambda a, b: a + b)\n",
    "    n_k_rdd = n_kv_rdd.map(lambda x: (x[0][1], x[1])).reduceByKey(lambda a, b: a + b)\n",
    "    n_d_rdd = doc_word_topic_rdd.map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    n_kv_map = n_kv_rdd.collectAsMap()\n",
    "    n_k_map = n_k_rdd.collectAsMap()\n",
    "    n_d_map = n_d_rdd.collectAsMap()\n",
    "    n_kv_broadcast = spark_context.broadcast(n_kv_map)\n",
    "    n_k_broadcast = spark_context.broadcast(n_k_map)\n",
    "    n_d_broadcast = spark_context.broadcast(n_d_map)\n",
    "    print(f\"  Initial counts collected and broadcasted (n_kv: {len(n_kv_map)}, n_k: {len(n_k_map)}, n_d: {len(n_d_map)})\")\n",
    "\n",
    "    broadcast_history = [(n_kv_broadcast, n_k_broadcast)]\n",
    "\n",
    "    print(f\"  Starting {iterations_local} Gibbs sampling iterations...\")\n",
    "    loop_start_time = time.time()\n",
    "    for i in range(iterations_local):\n",
    "        iter_start_time = time.time()\n",
    "        current_n_kv_broadcast, current_n_k_broadcast = broadcast_history[-1]\n",
    "\n",
    "        new_doc_word_topic_rdd = doc_word_topic_rdd.mapPartitions(\n",
    "            lambda p: sample_partition(p, K_local, alpha_local, beta_local, V_local,\n",
    "                                       current_n_kv_broadcast,\n",
    "                                       current_n_k_broadcast,\n",
    "                                       n_d_broadcast)\n",
    "        ).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        new_n_kv_rdd = new_doc_word_topic_rdd.map(lambda x: ((x[1], x[2]), 1)).reduceByKey(lambda a, b: a + b)\n",
    "        new_n_k_rdd = new_n_kv_rdd.map(lambda x: (x[0][1], x[1])).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "        new_n_kv_map = new_n_kv_rdd.collectAsMap()\n",
    "        new_n_k_map = new_n_k_rdd.collectAsMap()\n",
    "\n",
    "        n_kv_broadcast = spark_context.broadcast(new_n_kv_map)\n",
    "        n_k_broadcast = spark_context.broadcast(new_n_k_map)\n",
    "        broadcast_history.append((n_kv_broadcast, n_k_broadcast))\n",
    "\n",
    "        old_rdd_to_unpersist = doc_word_topic_rdd\n",
    "        doc_word_topic_rdd = new_doc_word_topic_rdd\n",
    "        old_rdd_to_unpersist.unpersist()\n",
    "\n",
    "        iter_duration = time.time() - iter_start_time\n",
    "        if (i + 1) % 10 == 0 or i == 0 or i == iterations_local - 1 :\n",
    "             print(f\"    Iteration {i+1}/{iterations_local} finished in {iter_duration:.2f} seconds. (n_kv size: {len(new_n_kv_map)})\")\n",
    "    loop_duration = time.time() - loop_start_time\n",
    "    print(f\"  Gibbs loop finished in {loop_duration:.2f} seconds.\")\n",
    "\n",
    "    final_n_kv_broadcast, final_n_k_broadcast = broadcast_history[-1]\n",
    "    print(\"  Extracting final Phi distribution...\")\n",
    "    final_n_kv = final_n_kv_broadcast.value\n",
    "    final_n_k = final_n_k_broadcast.value\n",
    "    phi_dist = {}\n",
    "    V_beta_term = V_local * beta_local\n",
    "    for k_idx in range(K_local):\n",
    "        phi_dist[k_idx] = {}\n",
    "        topic_total_words = final_n_k.get(k_idx, 0)\n",
    "        denominator = topic_total_words + V_beta_term\n",
    "        if denominator == 0: denominator = 1e-9\n",
    "        for v_idx in range(V_local):\n",
    "             word_topic_count = final_n_kv.get((v_idx, k_idx), 0)\n",
    "             phi_dist[k_idx][v_idx] = (word_topic_count + beta_local) / denominator\n",
    "\n",
    "    print(\"  Cleaning up final LDA RDD and intermediate broadcasts...\")\n",
    "    doc_word_topic_rdd.unpersist()\n",
    "    n_d_broadcast.destroy()\n",
    "    for i in range(len(broadcast_history)): # Clean all history including last one\n",
    "         kv_b, k_b = broadcast_history[i]\n",
    "         try:\n",
    "             kv_b.destroy(blocking=False) # Use non-blocking destroy\n",
    "             k_b.destroy(blocking=False)\n",
    "         except Exception as e:\n",
    "             print(f\"    Warn: Error destroying historical broadcast {i}: {e}\")\n",
    "\n",
    "    lda_internal_duration = time.time() - lda_internal_start_time\n",
    "    print(f\"LDA Gibbs run completed in {lda_internal_duration:.2f} seconds.\")\n",
    "    return phi_dist\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Section 2: Topic Coherence Calculation Functions\n",
    "# =================================================\n",
    "\n",
    "def get_relevant_word_pairs(words_in_doc, all_top_words_broadcast_local):\n",
    "    \"\"\"Generates pairs of unique words from a doc, filtering for top words.\"\"\"\n",
    "    unique_words_in_doc = set(words_in_doc)\n",
    "    relevant_top_words_in_doc = unique_words_in_doc.intersection(all_top_words_broadcast_local.value)\n",
    "    if len(relevant_top_words_in_doc) < 2: return []\n",
    "    pairs = []\n",
    "    for w1, w2 in combinations(sorted(list(relevant_top_words_in_doc)), 2):\n",
    "         pairs.append(((w1, w2), 1))\n",
    "    return pairs\n",
    "\n",
    "def calculate_topic_coherence(phi_dist, K_local, tokenized_rdd_local, N_local, V_local, num_top_words_coherence, spark_context):\n",
    "    \"\"\"Calculates average NPMI coherence for given topics.\"\"\"\n",
    "    print(f\"--- Calculating Coherence for K={K_local} ---\")\n",
    "    coherence_internal_start_time = time.time()\n",
    "\n",
    "    # 1.1 Get Top N Words per Topic (Strings)\n",
    "    print(f\"  Extracting top {num_top_words_coherence} words...\")\n",
    "    top_words_per_topic_strings = {}\n",
    "    for k in range(K_local):\n",
    "        sorted_words = sorted(phi_dist[k].items(), key=lambda item: item[1], reverse=True)\n",
    "        top_words_indices = [word_id for word_id, prob in sorted_words[:num_top_words_coherence]]\n",
    "        top_words_per_topic_strings[k] = [index_to_word_broadcast.value.get(idx, f\"UNKNOWN_IDX_{idx}\") for idx in top_words_indices]\n",
    "\n",
    "    # 1.2 Calculate Word Document Frequencies (Using original lemmatized tokens)\n",
    "    print(\"  Calculating document frequencies...\")\n",
    "    if not tokenized_rdd_local.is_cached:\n",
    "        tokenized_rdd_local.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        print(\"    INFO: tokenized_rdd_local was not cached, persisting now.\")\n",
    "\n",
    "    doc_unique_words_rdd = tokenized_rdd_local.mapValues(lambda words: list(set(words))).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    word_doc_counts_rdd = doc_unique_words_rdd.flatMap(lambda x: [(word, 1) for word in x[1]])\n",
    "    word_doc_freq_rdd = word_doc_counts_rdd.reduceByKey(lambda a, b: a + b)\n",
    "    action_start_time = time.time()\n",
    "    word_doc_freq_map = word_doc_freq_rdd.collectAsMap()\n",
    "    print(f\"    Collect word_doc_freq action took: {time.time() - action_start_time:.2f} seconds\")\n",
    "    word_doc_freq_broadcast = spark_context.broadcast(word_doc_freq_map)\n",
    "\n",
    "    # 1.3 Calculate Word Pair Co-Document Frequencies (Optimized)\n",
    "    print(\"  Calculating pair co-document frequencies...\")\n",
    "    all_top_words_set = set(w for words in top_words_per_topic_strings.values() for w in words)\n",
    "    all_top_words_broadcast = spark_context.broadcast(all_top_words_set)\n",
    "\n",
    "    word_pair_counts_rdd = tokenized_rdd_local.flatMap(\n",
    "        lambda doc_data: get_relevant_word_pairs(doc_data[1], all_top_words_broadcast)\n",
    "    )\n",
    "    word_pair_cocount_rdd = word_pair_counts_rdd.reduceByKey(lambda a, b: a + b)\n",
    "    action_start_time = time.time()\n",
    "    word_pair_cocount_map = word_pair_cocount_rdd.collectAsMap()\n",
    "    print(f\"    Collect word_pair_cocount action took: {time.time() - action_start_time:.2f} seconds\")\n",
    "    word_pair_cocount_broadcast = spark_context.broadcast(word_pair_cocount_map)\n",
    "    doc_unique_words_rdd.unpersist()\n",
    "\n",
    "    # 1.4 Calculate NPMI and Average Coherence per Topic\n",
    "    print(\"  Calculating NPMI scores...\")\n",
    "    topic_coherence_scores = {}\n",
    "    epsilon = 1e-12\n",
    "    doc_freqs = word_doc_freq_broadcast.value\n",
    "    pair_cocounts = word_pair_cocount_broadcast.value\n",
    "\n",
    "    overall_npmi_sum = 0\n",
    "    total_pairs_calculated = 0\n",
    "\n",
    "    for k in range(K_local):\n",
    "        topic_words = top_words_per_topic_strings[k]\n",
    "        npmi_topic_scores = []\n",
    "        print(f\"\\n    Topic {k} Top Words: {topic_words}\")\n",
    "\n",
    "        for i in range(num_top_words_coherence):\n",
    "            for j in range(i + 1, num_top_words_coherence):\n",
    "                w1 = topic_words[i]\n",
    "                w2 = topic_words[j]\n",
    "                count_w1 = doc_freqs.get(w1, 0)\n",
    "                count_w2 = doc_freqs.get(w2, 0)\n",
    "                pair_key = tuple(sorted((w1, w2)))\n",
    "                count_w1_w2 = pair_cocounts.get(pair_key, 0)\n",
    "\n",
    "                if count_w1 == 0 or count_w2 == 0 or count_w1_w2 == 0: npmi = 0.0\n",
    "                else:\n",
    "                    p_w1 = count_w1 / N_local\n",
    "                    p_w2 = count_w2 / N_local\n",
    "                    p_w1_w2 = count_w1_w2 / N_local\n",
    "                    pmi = math.log2((p_w1_w2 + epsilon) / (p_w1 * p_w2 + epsilon))\n",
    "                    denominator = -math.log2(p_w1_w2 + epsilon)\n",
    "                    if abs(denominator) < epsilon: npmi = 0.0\n",
    "                    else: npmi = pmi / denominator\n",
    "                    npmi = max(-1.0, min(1.0, npmi))\n",
    "                npmi_topic_scores.append(npmi)\n",
    "\n",
    "        if not npmi_topic_scores: topic_coherence_scores[k] = 0.0\n",
    "        else: topic_coherence_scores[k] = sum(npmi_topic_scores) / len(npmi_topic_scores)\n",
    "        print(f\"    Topic {k} Coherence (Avg. NPMI): {topic_coherence_scores[k]:.4f}\")\n",
    "        overall_npmi_sum += sum(npmi_topic_scores)\n",
    "        total_pairs_calculated += len(npmi_topic_scores)\n",
    "\n",
    "    average_coherence = overall_npmi_sum / total_pairs_calculated if total_pairs_calculated > 0 else 0.0\n",
    "    print(f\"\\n  Overall Average Coherence for K={K_local}: {average_coherence:.4f}\")\n",
    "\n",
    "    # Cleanup coherence calculation broadcasts\n",
    "    word_doc_freq_broadcast.destroy(blocking=False)\n",
    "    word_pair_cocount_broadcast.destroy(blocking=False)\n",
    "    all_top_words_broadcast.destroy(blocking=False)\n",
    "\n",
    "    coherence_internal_duration = time.time() - coherence_internal_start_time\n",
    "    print(f\"--- Coherence calculation finished in {coherence_internal_duration:.2f} seconds ---\")\n",
    "    return average_coherence\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Section 3: Hyperparameter Tuning Loop\n",
    "# =================================================\n",
    "\n",
    "print(\"\\n--- Starting Hyperparameter Tuning Loop (Varying K) ---\")\n",
    "tuning_overall_start_time = time.time()\n",
    "\n",
    "# --- Define Tuning Parameters ---\n",
    "k_values_to_test = [5, 15, 25, 27, 30] # Adjust range as needed\n",
    "\n",
    "base_alpha_heuristic = 0.1\n",
    "base_beta = 0.01\n",
    "base_iterations = 100 # Increase if needed, start with 100\n",
    "\n",
    "num_top_words_for_coherence = 10\n",
    "\n",
    "# --- Store results ---\n",
    "tuning_results = {}\n",
    "phi_results = {}\n",
    "\n",
    "# --- Main Tuning Loop ---\n",
    "for current_K in k_values_to_test:\n",
    "    print(f\"\\n===== Processing K = {current_K} =====\")\n",
    "    tuning_run_start_time = time.time()\n",
    "\n",
    "    # Determine alpha\n",
    "    if isinstance(base_alpha_heuristic, str) and base_alpha_heuristic.lower() == '50/k': current_alpha = 50.0 / current_K\n",
    "    elif isinstance(base_alpha_heuristic, str) and base_alpha_heuristic.lower() == '1/k': current_alpha = 1.0 / current_K\n",
    "    else: current_alpha = float(base_alpha_heuristic)\n",
    "    current_beta = base_beta\n",
    "    current_iterations = base_iterations\n",
    "\n",
    "    # --- Run LDA ---\n",
    "    # Uses filtered doc_word_tokens_rdd and filtered V\n",
    "    current_phi = run_lda_gibbs(doc_word_tokens_rdd, current_K, current_alpha, current_beta,\n",
    "                                current_iterations, V, N, spark.sparkContext)\n",
    "    phi_results[current_K] = current_phi\n",
    "\n",
    "    # --- Calculate Coherence ---\n",
    "    # Uses original lemmatized tokenized_rdd and filtered V\n",
    "    current_average_coherence = calculate_topic_coherence(current_phi, current_K, tokenized_rdd, N, V,\n",
    "                                                         num_top_words_for_coherence, spark.sparkContext)\n",
    "    tuning_results[current_K] = current_average_coherence\n",
    "\n",
    "    tuning_run_duration = time.time() - tuning_run_start_time\n",
    "    print(f\"===== Finished processing K = {current_K} in {tuning_run_duration:.2f} seconds =====\")\n",
    "\n",
    "\n",
    "# --- Analyze Tuning Results ---\n",
    "tuning_overall_duration = time.time() - tuning_overall_start_time\n",
    "print(\"\\n--- Tuning Complete ---\")\n",
    "print(f\"Total tuning time: {tuning_overall_duration:.2f} seconds\")\n",
    "print(\"\\nK vs. Average Coherence:\")\n",
    "sorted_k = sorted(tuning_results.keys())\n",
    "for k_val in sorted_k:\n",
    "    score = tuning_results.get(k_val, float('nan')) # Handle missing results if any\n",
    "    print(f\"  K={k_val}: {score:.4f}\")\n",
    "\n",
    "if tuning_results:\n",
    "    # Find best K based on highest coherence score\n",
    "    best_k = max(tuning_results, key=tuning_results.get)\n",
    "    best_coherence = tuning_results[best_k]\n",
    "    print(f\"\\nBest K found: {best_k} with coherence {best_coherence:.4f}\")\n",
    "\n",
    "    print(f\"\\n--- Top {num_top_words_for_coherence} Words for Best K={best_k} ---\")\n",
    "    best_phi = phi_results.get(best_k) # Get phi for the best K\n",
    "    if best_phi:\n",
    "        for k in range(best_k):\n",
    "             sorted_words = sorted(best_phi[k].items(), key=lambda item: item[1], reverse=True)\n",
    "             top_words_indices = [word_id for word_id, prob in sorted_words[:num_top_words_for_coherence]]\n",
    "             topic_words = [index_to_word_broadcast.value.get(idx, f\"UNKNOWN_IDX_{idx}\") for idx in top_words_indices]\n",
    "             print(f\"  Topic {k}: {topic_words}\")\n",
    "    else:\n",
    "        print(f\"    Could not retrieve Phi results for K={best_k}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo tuning results found.\")\n",
    "    best_k = None\n",
    "\n",
    "# Plot the results\n",
    "if tuning_results:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_k_values = sorted(tuning_results.keys())\n",
    "    plot_coherence_values = [tuning_results[k_val] for k_val in plot_k_values]\n",
    "    plt.plot(plot_k_values, plot_coherence_values, marker='o')\n",
    "\n",
    "    plt.xlabel(\"Number of Topics (K)\")\n",
    "    plt.ylabel(\"Average Topic Coherence (NPMI)\")\n",
    "    alpha_str = f\"{base_alpha_heuristic:.2f}\" if isinstance(base_alpha_heuristic, float) else base_alpha_heuristic\n",
    "    plt.title(f\"LDA Coherence vs. Number of Topics (K)\\n(alpha={alpha_str}, beta={base_beta}, iters={base_iterations})\")\n",
    "    plt.xticks(k_values_to_test)\n",
    "    plt.grid(True)\n",
    "    if best_k:\n",
    "        plt.scatter([best_k], [best_coherence], color='red', s=100, label=f'Best K={best_k} ({best_coherence:.3f})', zorder=5)\n",
    "        plt.legend()\n",
    "    plt.savefig(f\"coherence_k_plot_medium.png\")\n",
    "    plt.close()\n",
    "print(\"--- End of Script ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. implement and run scalability experiments\n",
    "## 5.1. Data size and scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Scalability Experiment (Time vs. Data Size) ---\n",
      "Using K = 35 for scalability tests.\n",
      "Using alpha=0.100, beta=0.01, iterations=100\n",
      "Testing data sizes: [5000, 10000, 15000, 18267]\n",
      "\n",
      "===== Processing Size = 5000 =====\n",
      "  Creating data subset...\n",
      "  Preprocessing subset (Target N = 5000)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Subset Vocab Size (V_subset): 11086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Created subset 'doc_word_tokens_rdd' with 626578 tokens.\n",
      "  Preprocessing subset took 6.65 secs.\n",
      "  Running LDA for subset (K=35, V=11086, N=5000)...\n",
      "Running LDA Gibbs: K=35, alpha=0.100, beta=0.01, iters=100\n",
      "  Initializing random topics...\n",
      "  Calculating initial counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Initial counts collected and broadcasted (n_kv: 180898, n_k: 35, n_d: 5133)\n",
      "  Starting 100 Gibbs sampling iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 1/100 finished in 12.87 seconds. (n_kv size: 145512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 10/100 finished in 12.01 seconds. (n_kv size: 70347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 20/100 finished in 11.96 seconds. (n_kv size: 56202)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 30/100 finished in 11.67 seconds. (n_kv size: 51945)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 40/100 finished in 11.80 seconds. (n_kv size: 50269)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 50/100 finished in 11.57 seconds. (n_kv size: 49515)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 60/100 finished in 11.63 seconds. (n_kv size: 49378)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 70/100 finished in 11.54 seconds. (n_kv size: 49207)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 80/100 finished in 11.55 seconds. (n_kv size: 49288)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 90/100 finished in 11.98 seconds. (n_kv size: 49455)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 100/100 finished in 11.69 seconds. (n_kv size: 49647)\n",
      "  Gibbs loop finished in 1179.37 seconds.\n",
      "  Extracting final Phi distribution...\n",
      "  Cleaning up final LDA RDD and intermediate broadcasts...\n",
      "LDA Gibbs run completed in 1180.95 seconds.\n",
      "  LDA run for size 5000 took: 1180.95 seconds\n",
      "  Cleaning up subset resources...\n",
      "  Subset cleanup attempted.\n",
      "===== Finished processing size = 5000 in 1187.61 seconds =====\n",
      "\n",
      "===== Processing Size = 10000 =====\n",
      "  Creating data subset...\n",
      "  Preprocessing subset (Target N = 10000)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Subset Vocab Size (V_subset): 17383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Created subset 'doc_word_tokens_rdd' with 1257539 tokens.\n",
      "  Preprocessing subset took 11.41 secs.\n",
      "  Running LDA for subset (K=35, V=17383, N=10000)...\n",
      "Running LDA Gibbs: K=35, alpha=0.100, beta=0.01, iters=100\n",
      "  Initializing random topics...\n",
      "  Calculating initial counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Initial counts collected and broadcasted (n_kv: 285156, n_k: 35, n_d: 10110)\n",
      "  Starting 100 Gibbs sampling iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 1/100 finished in 26.59 seconds. (n_kv size: 231042)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration 10/100 finished in 25.29 seconds. (n_kv size: 114902)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3728:>                                                       (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Section 4: Scalability Experiment (Time vs. Data Size)\n",
    "# ==============================================================================\n",
    "# NOTE: This section assumes the previous consolidated block (Sections 0-3)\n",
    "# has run successfully and the following are defined:\n",
    "#   - spark: SparkSession\n",
    "#   - df: Original DataFrame\n",
    "#   - N: Total number of documents in original df\n",
    "#   - best_k: The optimal K found from tuning (e.g., 15)\n",
    "#   - base_alpha_heuristic, base_beta, base_iterations: Parameters used for tuning\n",
    "#   - tokenize_and_lemmatize: The preprocessing function\n",
    "#   - min_df, max_df_ratio: Vocabulary filtering parameters used before\n",
    "#   - run_lda_gibbs: The Gibbs sampling function\n",
    "#   - plt: matplotlib.pyplot\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Starting Scalability Experiment (Time vs. Data Size) ---\")\n",
    "scalability_results = {} # Stores { data_size: lda_execution_time }\n",
    "overall_scalability_start_time = time.time()\n",
    "\n",
    "# --- Define Parameters for Scalability Runs ---\n",
    "# Use the best K found during tuning\n",
    "if 'best_k' not in globals() or best_k is None:\n",
    "    print(\"WARN: 'best_k' not found from tuning, using K=15 as default for scalability.\")\n",
    "    best_k_final = 15\n",
    "else:\n",
    "    best_k_final = best_k\n",
    "print(f\"Using K = {best_k_final} for scalability tests.\")\n",
    "\n",
    "# Use the alpha/beta/iterations from the tuning section\n",
    "# Determine final alpha\n",
    "if isinstance(base_alpha_heuristic, str) and base_alpha_heuristic.lower() == '50/k':\n",
    "    alpha_final = 50.0 / best_k_final\n",
    "elif isinstance(base_alpha_heuristic, str) and base_alpha_heuristic.lower() == '1/k':\n",
    "    alpha_final = 1.0 / best_k_final\n",
    "else: # Assume it's a fixed numeric value\n",
    "    alpha_final = float(base_alpha_heuristic)\n",
    "beta_final = base_beta\n",
    "iterations_final = base_iterations\n",
    "print(f\"Using alpha={alpha_final:.3f}, beta={beta_final}, iterations={iterations_final}\")\n",
    "\n",
    "# --- Define Data Sizes to Test ---\n",
    "# Make sure N (full size) is included, usually last\n",
    "data_sizes_to_test = [5000, 10000, 15000, N]\n",
    "# Filter out sizes larger than N if N is small\n",
    "data_sizes_to_test = [s for s in data_sizes_to_test if s <= N]\n",
    "if N not in data_sizes_to_test: # Ensure full dataset is tested\n",
    "    data_sizes_to_test.append(N)\n",
    "data_sizes_to_test = sorted(list(set(data_sizes_to_test))) # Unique sorted sizes\n",
    "print(f\"Testing data sizes: {data_sizes_to_test}\")\n",
    "\n",
    "\n",
    "# --- Loop Through Data Sizes ---\n",
    "for current_size in data_sizes_to_test:\n",
    "    print(f\"\\n===== Processing Size = {current_size} =====\")\n",
    "    run_start_time = time.time()\n",
    "    subset_prep_success = False # Flag to track if prep finished\n",
    "\n",
    "    try:\n",
    "        # 1. Create Subset of ORIGINAL df\n",
    "        print(f\"  Creating data subset...\")\n",
    "        if current_size == N:\n",
    "             df_subset = df # Use full df for the last run\n",
    "             N_subset = N\n",
    "        else:\n",
    "             # Calculate fraction carefully to avoid issues if current_size > N\n",
    "             fraction = min(1.0, current_size / N)\n",
    "             df_subset = df.sample(False, fraction, seed=42)\n",
    "             # It's better to rely on the target size rather than the exact sampled count\n",
    "             N_subset = current_size # Use the target size for calculations like max_df\n",
    "\n",
    "        # 2. Re-run Preprocessing for the Subset\n",
    "        print(f\"  Preprocessing subset (Target N = {N_subset})...\")\n",
    "        prep_subset_start_time = time.time()\n",
    "\n",
    "        # Re-apply tokenization/lemmatization\n",
    "        # Use the globally defined tokenize_udf\n",
    "        tokenized_rdd_subset = df_subset.select(\"doc_id\", tokenize_udf(col(\"Text\")).alias(\"tokens\")) \\\n",
    "                                 .rdd.map(lambda row: (row.doc_id, row.tokens))\n",
    "        tokenized_rdd_subset.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        tokenized_rdd_subset.count() # Action\n",
    "\n",
    "        # Recalculate Doc Frequencies for this subset\n",
    "        doc_unique_words_rdd_subset = tokenized_rdd_subset.mapValues(lambda words: list(set(words))).cache()\n",
    "        word_doc_counts_rdd_subset = doc_unique_words_rdd_subset.flatMap(lambda x: [(word, 1) for word in x[1]])\n",
    "        word_doc_freq_rdd_subset = word_doc_counts_rdd_subset.reduceByKey(lambda a, b: a + b).cache()\n",
    "\n",
    "        # Filter vocabulary based on this subset's frequencies\n",
    "        max_doc_count_subset = N_subset * max_df_ratio\n",
    "        filtered_word_doc_freq_rdd_subset = word_doc_freq_rdd_subset.filter(\n",
    "            lambda wc: wc[1] >= min_df and wc[1] <= max_doc_count_subset\n",
    "        )\n",
    "\n",
    "        # Create new Vocab maps for the subset\n",
    "        filtered_vocabulary_rdd_subset = filtered_word_doc_freq_rdd_subset.map(lambda x: x[0]).zipWithIndex()\n",
    "        V_subset = filtered_vocabulary_rdd_subset.count() # Subset vocabulary size\n",
    "        print(f\"    Subset Vocab Size (V_subset): {V_subset}\")\n",
    "        if V_subset == 0:\n",
    "             print(\"    WARN: Subset vocabulary empty, skipping LDA run for this size.\")\n",
    "             scalability_results[current_size] = float('nan')\n",
    "             raise StopIteration(\"Empty Vocabulary\") # Use StopIteration to break out of try\n",
    "\n",
    "        vocab_map_subset = filtered_vocabulary_rdd_subset.collectAsMap()\n",
    "        vocab_broadcast_subset = spark.sparkContext.broadcast(vocab_map_subset)\n",
    "\n",
    "        # Filter tokens based on subset vocabulary\n",
    "        filtered_tokenized_rdd_subset = tokenized_rdd_subset.mapValues(\n",
    "            lambda tokens: [token for token in tokens if token in vocab_broadcast_subset.value]\n",
    "        ).cache()\n",
    "\n",
    "        # Define UDF specific to this subset's broadcast\n",
    "        def get_filtered_word_id_subset(token): return vocab_broadcast_subset.value.get(token, -1)\n",
    "        get_filtered_word_id_udf_subset = udf(get_filtered_word_id_subset, IntegerType())\n",
    "\n",
    "        # Recalculate word counts based on filtered tokens\n",
    "        temp_word_counts_rdd_subset = filtered_tokenized_rdd_subset.flatMap(\n",
    "            lambda x: [( (x[0], token), 1 ) for token in x[1]]\n",
    "        ).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "        word_counts_df_filtered_subset = temp_word_counts_rdd_subset.map(\n",
    "            lambda x: Row(doc_id=x[0][0], token=x[0][1], local_count=x[1])\n",
    "        ).toDF().withColumn(\"word_id\", get_filtered_word_id_udf_subset(col(\"token\"))) \\\n",
    "               .select(\"doc_id\", \"word_id\", \"local_count\") \\\n",
    "               .filter(col(\"word_id\") != -1)\n",
    "        word_counts_df_filtered_subset.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        # Create final input RDD for LDA for this subset\n",
    "        doc_word_tokens_rdd_subset = word_counts_df_filtered_subset.rdd.flatMap(\n",
    "            lambda row: [(row.doc_id, row.word_id)] * row.local_count\n",
    "        )\n",
    "        doc_word_tokens_rdd_subset.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        num_tokens_subset = doc_word_tokens_rdd_subset.count() # Action\n",
    "\n",
    "        print(f\"    Created subset 'doc_word_tokens_rdd' with {num_tokens_subset} tokens.\")\n",
    "        print(f\"  Preprocessing subset took {time.time() - prep_subset_start_time:.2f} secs.\")\n",
    "        subset_prep_success = True # Mark prep as successful\n",
    "\n",
    "        # 3. Time LDA Run for the Subset\n",
    "        print(f\"  Running LDA for subset (K={best_k_final}, V={V_subset}, N={N_subset})...\")\n",
    "        lda_timing_start = time.time()\n",
    "\n",
    "        # Call run_lda_gibbs with subset's RDD, V, N and BEST fixed K, alpha, beta, iters\n",
    "        # NOTE: run_lda_gibbs uses V_local and N_local arguments passed to it\n",
    "        _ = run_lda_gibbs(doc_word_tokens_rdd_subset, best_k_final, alpha_final, beta_final,\n",
    "                          iterations_final, V_subset, N_subset, spark.sparkContext)\n",
    "\n",
    "        lda_time = time.time() - lda_timing_start\n",
    "        print(f\"  LDA run for size {current_size} took: {lda_time:.2f} seconds\")\n",
    "        scalability_results[current_size] = lda_time\n",
    "\n",
    "    except StopIteration as si: # Catch the empty vocab exception\n",
    "        print(f\"    Skipping K={current_size} due to {si}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing size {current_size}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for debugging\n",
    "        scalability_results[current_size] = float('nan') # Mark as failed\n",
    "    finally:\n",
    "        # 4. Cleanup Subset Resources (always attempt cleanup)\n",
    "        print(\"  Cleaning up subset resources...\")\n",
    "        # Use try-except for each unpersist/destroy in case RDDs weren't created due to error\n",
    "        try: tokenized_rdd_subset.unpersist()\n",
    "        except NameError: pass\n",
    "        try: doc_unique_words_rdd_subset.unpersist()\n",
    "        except NameError: pass\n",
    "        try: word_doc_freq_rdd_subset.unpersist()\n",
    "        except NameError: pass\n",
    "        try: vocab_broadcast_subset.destroy(blocking=False)\n",
    "        except NameError: pass\n",
    "        try: filtered_tokenized_rdd_subset.unpersist()\n",
    "        except NameError: pass\n",
    "        try: word_counts_df_filtered_subset.unpersist()\n",
    "        except NameError: pass\n",
    "        try: doc_word_tokens_rdd_subset.unpersist()\n",
    "        except NameError: pass\n",
    "        print(\"  Subset cleanup attempted.\")\n",
    "\n",
    "        run_duration = time.time() - run_start_time\n",
    "        print(f\"===== Finished processing size = {current_size} in {run_duration:.2f} seconds =====\")\n",
    "\n",
    "\n",
    "# --- Plot Scalability Results ---\n",
    "print(\"\\n--- Scalability Results (Time vs. Data Size) ---\")\n",
    "if scalability_results:\n",
    "     # Filter out failed runs (nan) before plotting\n",
    "     valid_sizes = [s for s in data_sizes_to_test if s in scalability_results and not math.isnan(scalability_results[s])]\n",
    "     valid_times = [scalability_results[s] for s in valid_sizes]\n",
    "\n",
    "     print(\"Size vs. LDA Time:\")\n",
    "     for size, t in zip(valid_sizes, valid_times):\n",
    "          print(f\"  Size={size}: {t:.2f} seconds\")\n",
    "\n",
    "     if valid_sizes: # Check if there are any valid results to plot\n",
    "         plt.figure(figsize=(10, 6))\n",
    "         plt.plot(valid_sizes, valid_times, marker='o')\n",
    "         plt.xlabel(\"Number of Documents (N)\")\n",
    "         plt.ylabel(\"LDA Execution Time (seconds)\")\n",
    "         plt.title(f\"Scalability: LDA Time vs. Data Size\\n(K={best_k_final}, alpha={alpha_final:.2f}, beta={beta_final}, iters={iterations_final})\")\n",
    "         plt.grid(True)\n",
    "         # Set x-axis ticks to the actual sizes tested\n",
    "         plt.xticks(valid_sizes)\n",
    "         # Optionally set y-axis limit if needed\n",
    "         # plt.ylim(bottom=0)\n",
    "         plt.savefig(f\"scalability_plot_medium.png\")\n",
    "         plt.close()\n",
    "     else:\n",
    "         print(\"No valid scalability results to plot.\")\n",
    "else:\n",
    "     print(\"No scalability results recorded.\")\n",
    "\n",
    "overall_scalability_duration = time.time() - overall_scalability_start_time\n",
    "print(f\"\\nTotal Scalability Experiment Time: {overall_scalability_duration:.2f} seconds\")\n",
    "print(\"--- End Scalability Experiment ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. MLlib Baseline Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Section 5: MLlib Baseline Comparison\n",
    "# ===========================================================\n",
    "# Assumes the main analysis notebook (tuning) has run and defines:\n",
    "# - spark: SparkSession\n",
    "# - V: Filtered Vocabulary size (from Section 0)\n",
    "# - word_counts_df_filtered: DataFrame with 'doc_id', 'word_id', 'local_count' (from Section 0)\n",
    "#   (Ensure this DF is still available/cached, or recreate if needed)\n",
    "# - best_k: The best K found from tuning (Section 3)\n",
    "# - base_iterations: The number of iterations used in tuning (Section 3)\n",
    "# - index_to_word_broadcast: Broadcast mapping {index: 'word'} (from Section 0)\n",
    "# - scalability_results: Dictionary from Time vs Size experiment (Section 4)\n",
    "# - N: Total original document count (from initial setup)\n",
    "# ===========================================================\n",
    "print(\"\\n--- Starting MLlib Baseline Comparison ---\")\n",
    "baseline_start_time = time.time()\n",
    "\n",
    "# Import necessary MLlib components\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA as MLlibLDA\n",
    "import pyspark.sql.functions as F\n",
    "import time # Ensure time is imported if running this cell independently later\n",
    "import math # Ensure math is imported\n",
    "\n",
    "# --- Define Parameters for MLlib Run ---\n",
    "# Define BEST_ITERATIONS based on the value used in the successful tuning run\n",
    "if 'base_iterations' in globals():\n",
    "    BEST_ITERATIONS = base_iterations\n",
    "else:\n",
    "    print(\"WARN: 'base_iterations' not found from tuning context, using 100 as default for MLlib maxIter.\")\n",
    "    BEST_ITERATIONS = 100 # Fallback default\n",
    "\n",
    "# Ensure best_k is defined\n",
    "if 'best_k' not in globals() or best_k is None:\n",
    "    print(\"WARN: 'best_k' not found from tuning, using 15 as default for MLlib k.\")\n",
    "    best_k = 15 # Fallback if needed\n",
    "\n",
    "# Ensure V (Vocabulary size from filtered prep) is defined\n",
    "if 'V' not in globals() or V is None:\n",
    "     raise NameError(\"ERROR: Filtered vocabulary size 'V' is not defined. Please ensure Section 0 ran.\")\n",
    "\n",
    "# Ensure word_counts_df_filtered is defined\n",
    "if 'word_counts_df_filtered' not in globals():\n",
    "     raise NameError(\"ERROR: 'word_counts_df_filtered' DataFrame is not defined. Please ensure Section 0 ran.\")\n",
    "\n",
    "# Ensure index_to_word_broadcast is defined\n",
    "if 'index_to_word_broadcast' not in globals():\n",
    "     raise NameError(\"ERROR: 'index_to_word_broadcast' is not defined. Please ensure Section 0 ran.\")\n",
    "\n",
    "mllib_lda_time_recorded = float('nan') # Initialize time result\n",
    "\n",
    "try:\n",
    "    # 1. Prepare Data for MLlib LDA\n",
    "    print(f\"  Preparing data for MLlib LDA (k={best_k}, V={V})...\")\n",
    "    prep_mllib_start_time = time.time()\n",
    "\n",
    "    # Ensure the input DataFrame is cached before potentially heavy groupBy/RDD conversion\n",
    "    if not word_counts_df_filtered.is_cached:\n",
    "         print(\"  WARN: word_counts_df_filtered not cached, might be slow. Re-caching.\")\n",
    "         word_counts_df_filtered.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "         word_counts_df_filtered.count() # Action to force caching\n",
    "\n",
    "    # Group counts by document and create sparse vectors\n",
    "    # Using RDD map is often more flexible for vector creation\n",
    "    mllib_input_df = word_counts_df_filtered \\\n",
    "        .groupBy(\"doc_id\") \\\n",
    "        .agg(collect_list(struct(col(\"word_id\"), col(\"local_count\"))).alias(\"counts\")) \\\n",
    "        .rdd \\\n",
    "        .map(lambda row: (row.doc_id, Vectors.sparse(V, sorted([(int(c.word_id), float(c.local_count)) for c in row.counts])))) \\\n",
    "        .toDF([\"doc_id\", \"features\"]) # MLlib expects \"features\" column\n",
    "\n",
    "    mllib_input_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    num_mllib_docs = mllib_input_df.count() # Action to materialize and cache\n",
    "    print(f\"  Created MLlib input DataFrame with {num_mllib_docs} documents.\")\n",
    "    print(f\"  MLlib data preparation took {time.time() - prep_mllib_start_time:.2f} seconds.\")\n",
    "\n",
    "    # 2. Run MLlib LDA and Time .fit()\n",
    "    print(f\"  Running MLlib LDA (k={best_k}, maxIter={BEST_ITERATIONS})...\")\n",
    "    # MLlib's EM optimizer is default and often faster than online for batch\n",
    "    mllib_lda = MLlibLDA(k=best_k, maxIter=BEST_ITERATIONS, optimizer='em', seed=42) # Use seed\n",
    "\n",
    "    mllib_timing_start = time.time()\n",
    "    mllib_model = mllib_lda.fit(mllib_input_df)\n",
    "    mllib_time = time.time() - mllib_timing_start\n",
    "\n",
    "    print(f\"  MLlib LDA .fit() took: {mllib_time:.2f} seconds\")\n",
    "    mllib_lda_time_recorded = mllib_time # Record successful time\n",
    "\n",
    "    # 3. Optional: Display MLlib Topics\n",
    "    print(f\"\\n  --- Top 10 Words for MLlib LDA (K={best_k}) ---\")\n",
    "    try:\n",
    "        topics = mllib_model.describeTopics(10)\n",
    "        # Need the index_to_word mapping\n",
    "        topic_summary = topics.rdd.map(lambda row: (row.topic, [index_to_word_broadcast.value.get(idx, f\"UNK_{idx}\") for idx in row.termIndices])).collect()\n",
    "        for topic_idx, words in sorted(topic_summary):\n",
    "            print(f\"    MLlib Topic {topic_idx}: {words}\")\n",
    "    except Exception as desc_e:\n",
    "        print(f\"    WARN: Could not describe MLlib topics: {desc_e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!!!!! ERROR during MLlib Baseline: {e} !!!!!\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Cleanup MLlib DataFrame\n",
    "    try:\n",
    "         mllib_input_df.unpersist()\n",
    "         print(\"  Unpersisted MLlib input DataFrame.\")\n",
    "    except NameError: pass # If it wasn't created due to error\n",
    "\n",
    "baseline_duration = time.time() - baseline_start_time\n",
    "print(f\"--- MLlib Baseline Comparison finished in {baseline_duration:.2f} seconds ---\")\n",
    "\n",
    "# 4. Display Performance Comparison\n",
    "# Requires 'scalability_results' and 'N' from the previous cell (Section 4)\n",
    "if 'scalability_results' in globals() and N in scalability_results and not math.isnan(mllib_lda_time_recorded):\n",
    "     my_lda_time = scalability_results[N]\n",
    "     print(\"\\n--- Performance Comparison (Full Dataset, LDA part only) ---\")\n",
    "     print(f\"  Your Gibbs LDA (K={best_k}, iters={base_iterations}): {my_lda_time:.2f} seconds\")\n",
    "     print(f\"  MLlib EM LDA (K={best_k}, maxIter={BEST_ITERATIONS}):  {mllib_lda_time_recorded:.2f} seconds\")\n",
    "     if mllib_lda_time_recorded > 0:\n",
    "         ratio = my_lda_time / mllib_lda_time_recorded\n",
    "         print(f\"  Ratio (Your Time / MLlib Time): {ratio:.2f}\")\n",
    "     else:\n",
    "         print(\"  Cannot calculate ratio (MLlib time is zero or invalid).\")\n",
    "elif 'scalability_results' not in globals() or N not in scalability_results:\n",
    "     print(\"\\nCould not perform final time comparison: Scalability results missing.\")\n",
    "elif math.isnan(mllib_lda_time_recorded):\n",
    "     print(\"\\nCould not perform final time comparison: MLlib baseline run failed.\")\n",
    "else:\n",
    "     print(\"\\nCould not perform final time comparison due to missing data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a4bb8c9aa6a45a8982534314cd94d52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11ff77e9d1154764908fc9eb8701c654": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1461cc1e0b4148219b70880efd08062b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36d9b977f1874b4b9a7f045bb0d28934": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4235614e07f94fb09327032c0cb9dfce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "437dc5deaf364f8fa25eb6903628fbe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9e5f8dcf817492fac520196361e6f71",
      "placeholder": "",
      "style": "IPY_MODEL_cc849e8f56964e968258ddd860aa7d55",
      "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
     }
    },
    "4ca5bf6ad29e48dfb037dc264b868ac4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57ddbab244c54f3d95611cb1303b6fa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_7c7a85efc3fa4d38b74205b778715c0b",
      "style": "IPY_MODEL_8357cd75779d4ad4bb8a19d96fdc191a",
      "tooltip": ""
     }
    },
    "5c683459d2064e85bb304120f7b9d495": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60b4742936cf47dabda75f67be093bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6293dcad80204bf7a93af3d3d686a1d8",
      "placeholder": "",
      "style": "IPY_MODEL_4235614e07f94fb09327032c0cb9dfce",
      "value": "Connecting..."
     }
    },
    "6293dcad80204bf7a93af3d3d686a1d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "658c54cfa30e4fa4a8b6763cf5eb7981": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d2561616e624701813a55bb8411759e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c83ff8c78e5496dbf92c311cfe38607"
      ],
      "layout": "IPY_MODEL_c62f8417df8743a29678cc3ad7d9d79d"
     }
    },
    "7c7a85efc3fa4d38b74205b778715c0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8357cd75779d4ad4bb8a19d96fdc191a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "86eeeb6140364be7b9b46f6b9af18840": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Username:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_36d9b977f1874b4b9a7f045bb0d28934",
      "placeholder": "",
      "style": "IPY_MODEL_11ff77e9d1154764908fc9eb8701c654",
      "value": "siomethinglee"
     }
    },
    "8c83ff8c78e5496dbf92c311cfe38607": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1461cc1e0b4148219b70880efd08062b",
      "placeholder": "",
      "style": "IPY_MODEL_5c683459d2064e85bb304120f7b9d495",
      "value": "Kaggle credentials successfully validated."
     }
    },
    "931157aaccb341bc848b0a320741de3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_658c54cfa30e4fa4a8b6763cf5eb7981",
      "placeholder": "",
      "style": "IPY_MODEL_4ca5bf6ad29e48dfb037dc264b868ac4",
      "value": ""
     }
    },
    "bee25b47df104aaabe3f6235b31d4370": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a4bb8c9aa6a45a8982534314cd94d52",
      "placeholder": "",
      "style": "IPY_MODEL_e9a4c6ed8d5846c8aa2c3f6cbbd4317a",
      "value": "\n<b>Thank You</b></center>"
     }
    },
    "c62f8417df8743a29678cc3ad7d9d79d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "cc849e8f56964e968258ddd860aa7d55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9a4c6ed8d5846c8aa2c3f6cbbd4317a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9e5f8dcf817492fac520196361e6f71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
