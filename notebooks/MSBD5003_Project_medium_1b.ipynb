{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3417ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration ---\n",
      "Testing Core Counts: [1, 2, 4, 8]\n",
      "Using K=19, Alpha=0.1, Beta=0.01, Iterations=100\n",
      "Using min_df=5, max_df_ratio=0.85\n",
      "Spark Memory Config: 32g\n",
      "--------------------\n",
      "Checking NLTK resources...\n",
      "NLTK resources ready.\n",
      "\n",
      "===== Testing with 1 Core(s) =====\n",
      "  Spark Session created for local[1]\n",
      "  Preprocessing full dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 12:57:56 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/12 12:57:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/12 12:57:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/12 12:57:56 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Full Vocab Size (V_full): 25666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preprocessing took 19.71 seconds.\n",
      "  Running LDA with 1 core(s)...\n",
      "  Running LDA Gibbs: K=19, alpha=0.100, beta=0.01, iters=100\n",
      "    Initializing random topics...\n",
      "    Calculating initial counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Initial counts collected and broadcasted (n_kv: 296469, n_k: 19, n_d: 18257)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting 100 Gibbs sampling iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 1/100 (36.92s). n_kv size: 248480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 20/100 (33.43s). n_kv size: 102616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 301:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 40/100 (33.11s). n_kv size: 90183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 441:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 60/100 (32.52s). n_kv size: 88928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 581:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 80/100 (32.96s). n_kv size: 89167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 721:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 100/100 (33.24s). n_kv size: 89775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Gibbs loop finished in 3325.09 seconds.\n",
      "    Cleaning up LDA RDDs and broadcasts...\n",
      "  LDA Gibbs run completed in 3329.04 seconds.\n",
      "  LDA Run Time for 1 core(s): 3329.04 seconds\n",
      "  Cleaning up resources for 1 core(s)...\n",
      "    Unpersisted tokenized_rdd\n",
      "    Unpersisted doc_unique_words_rdd\n",
      "    Unpersisted word_doc_freq_rdd\n",
      "    Destroyed vocab_broadcast\n",
      "    Unpersisted filtered_tokenized_rdd\n",
      "    Unpersisted word_counts_df_filtered\n",
      "    Unpersisted doc_word_tokens_rdd\n",
      "    Stopped Spark session.\n",
      "===== Finished testing 1 core(s) in 3352.81 seconds =====\n",
      "\n",
      "===== Testing with 2 Core(s) =====\n",
      "  Spark Session created for local[2]\n",
      "  Preprocessing full dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 13:53:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/12 13:53:49 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/12 13:53:49 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Full Vocab Size (V_full): 25666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preprocessing took 19.09 seconds.\n",
      "  Running LDA with 2 core(s)...\n",
      "  Running LDA Gibbs: K=19, alpha=0.100, beta=0.01, iters=100\n",
      "    Initializing random topics...\n",
      "    Calculating initial counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Initial counts collected and broadcasted (n_kv: 296628, n_k: 19, n_d: 18257)\n",
      "    Starting 100 Gibbs sampling iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 1/100 (35.39s). n_kv size: 248321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 20/100 (33.53s). n_kv size: 104478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 301:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 40/100 (33.02s). n_kv size: 89752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 441:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 60/100 (33.06s). n_kv size: 87809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 581:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 80/100 (33.42s). n_kv size: 87619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 721:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 100/100 (32.44s). n_kv size: 87707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Gibbs loop finished in 3328.27 seconds.\n",
      "    Cleaning up LDA RDDs and broadcasts...\n",
      "  LDA Gibbs run completed in 3331.83 seconds.\n",
      "  LDA Run Time for 2 core(s): 3331.83 seconds\n",
      "  Cleaning up resources for 2 core(s)...\n",
      "    Unpersisted tokenized_rdd\n",
      "    Unpersisted doc_unique_words_rdd\n",
      "    Unpersisted word_doc_freq_rdd\n",
      "    Destroyed vocab_broadcast\n",
      "    Unpersisted filtered_tokenized_rdd\n",
      "    Unpersisted word_counts_df_filtered\n",
      "    Unpersisted doc_word_tokens_rdd\n",
      "    Stopped Spark session.\n",
      "===== Finished testing 2 core(s) in 3354.95 seconds =====\n",
      "\n",
      "===== Testing with 4 Core(s) =====\n",
      "  Spark Session created for local[4]\n",
      "  Preprocessing full dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 14:49:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/12 14:49:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/12 14:49:44 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Full Vocab Size (V_full): 25666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preprocessing took 19.10 seconds.\n",
      "  Running LDA with 4 core(s)...\n",
      "  Running LDA Gibbs: K=19, alpha=0.100, beta=0.01, iters=100\n",
      "    Initializing random topics...\n",
      "    Calculating initial counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Initial counts collected and broadcasted (n_kv: 296846, n_k: 19, n_d: 18257)\n",
      "    Starting 100 Gibbs sampling iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 1/100 (34.69s). n_kv size: 248429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 20/100 (32.58s). n_kv size: 102955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 301:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 40/100 (35.77s). n_kv size: 90489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 441:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 60/100 (33.00s). n_kv size: 88402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 581:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 80/100 (35.40s). n_kv size: 88230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 100/100 (34.00s). n_kv size: 88385\n",
      "    Gibbs loop finished in 3358.12 seconds.\n",
      "    Cleaning up LDA RDDs and broadcasts...\n",
      "  LDA Gibbs run completed in 3361.74 seconds.\n",
      "  LDA Run Time for 4 core(s): 3361.74 seconds\n",
      "  Cleaning up resources for 4 core(s)...\n",
      "    Unpersisted tokenized_rdd\n",
      "    Unpersisted doc_unique_words_rdd\n",
      "    Unpersisted word_doc_freq_rdd\n",
      "    Destroyed vocab_broadcast\n",
      "    Unpersisted filtered_tokenized_rdd\n",
      "    Unpersisted word_counts_df_filtered\n",
      "    Unpersisted doc_word_tokens_rdd\n",
      "    Stopped Spark session.\n",
      "===== Finished testing 4 core(s) in 3384.86 seconds =====\n",
      "\n",
      "===== Testing with 8 Core(s) =====\n",
      "  Spark Session created for local[8]\n",
      "  Preprocessing full dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 15:46:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/12 15:46:08 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/12 15:46:08 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Full Vocab Size (V_full): 25666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preprocessing took 23.90 seconds.\n",
      "  Running LDA with 8 core(s)...\n",
      "  Running LDA Gibbs: K=19, alpha=0.100, beta=0.01, iters=100\n",
      "    Initializing random topics...\n",
      "    Calculating initial counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Initial counts collected and broadcasted (n_kv: 296325, n_k: 19, n_d: 18257)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting 100 Gibbs sampling iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 1/100 (40.10s). n_kv size: 248196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 20/100 (35.37s). n_kv size: 104750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 301:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 40/100 (35.42s). n_kv size: 91033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 60/100 (36.92s). n_kv size: 88805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 581:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 80/100 (35.27s). n_kv size: 88573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 721:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter 100/100 (35.81s). n_kv size: 88590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Gibbs loop finished in 3582.82 seconds.\n",
      "    Cleaning up LDA RDDs and broadcasts...\n",
      "  LDA Gibbs run completed in 3586.93 seconds.\n",
      "  LDA Run Time for 8 core(s): 3586.93 seconds\n",
      "  Cleaning up resources for 8 core(s)...\n",
      "    Unpersisted tokenized_rdd\n",
      "    Unpersisted doc_unique_words_rdd\n",
      "    Unpersisted word_doc_freq_rdd\n",
      "    Destroyed vocab_broadcast\n",
      "    Unpersisted filtered_tokenized_rdd\n",
      "    Unpersisted word_counts_df_filtered\n",
      "    Unpersisted doc_word_tokens_rdd\n",
      "    Stopped Spark session.\n",
      "===== Finished testing 8 core(s) in 3614.84 seconds =====\n",
      "\n",
      "--- Executor Scaling Results (Time vs. Cores) ---\n",
      "Cores vs. LDA Time:\n",
      "  Cores=1: 3329.04 seconds\n",
      "  Cores=2: 3331.83 seconds\n",
      "  Cores=4: 3361.74 seconds\n",
      "  Cores=8: 3586.93 seconds\n",
      "\n",
      "Cores vs. Speedup:\n",
      "  Cores=1: 1.00x\n",
      "  Cores=2: 1.00x\n",
      "  Cores=4: 0.99x\n",
      "  Cores=8: 0.93x\n",
      "--- End Executor Scaling Experiment ---\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# NEW NOTEBOOK: LDA Executor Scaling Test\n",
    "# ===========================================================\n",
    "# Purpose: Test LDA runtime scaling with varying Spark cores (local mode)\n",
    "# Instructions:\n",
    "# 1. Save this entire code block as a new .ipynb file (e.g., lda_executor_scaling.ipynb).\n",
    "# 2. Place the data file 'df_file.csv' in the same directory.\n",
    "# 3. UPDATE the BEST_K, BEST_ALPHA, etc. constants below with your tuning results.\n",
    "# 4. Adjust CORE_COUNTS_TO_TEST based on your machine's cores.\n",
    "# 5. Run the entire notebook. It will stop/start Spark multiple times.\n",
    "# ===========================================================\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys # Import sys for flushing output\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, udf\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType, StructType, StructField, StringType\n",
    "from pyspark import StorageLevel\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "# *** UPDATE THESE with your best parameters found from tuning ***\n",
    "BEST_K = 24\n",
    "BEST_ALPHA = 0.1\n",
    "BEST_BETA = 0.01\n",
    "BEST_ITERATIONS = 100\n",
    "# **************************************************************\n",
    "\n",
    "CORE_COUNTS_TO_TEST = [1, 2, 4, 8] # Adjust based on your machine's cores\n",
    "DATA_FILE_PATH = \"df_Mid_Size.csv\" # Path to your data\n",
    "# Use preprocessing params consistent with tuning run\n",
    "MIN_DF = 5\n",
    "MAX_DF_RATIO = 0.85\n",
    "# Set driver/executor memory (adjust based on your machine's RAM)\n",
    "SPARK_MEMORY = \"32g\"\n",
    "\n",
    "print(\"--- Configuration ---\")\n",
    "print(f\"Testing Core Counts: {CORE_COUNTS_TO_TEST}\")\n",
    "print(f\"Using K={BEST_K}, Alpha={BEST_ALPHA}, Beta={BEST_BETA}, Iterations={BEST_ITERATIONS}\")\n",
    "print(f\"Using min_df={MIN_DF}, max_df_ratio={MAX_DF_RATIO}\")\n",
    "print(f\"Spark Memory Config: {SPARK_MEMORY}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "\n",
    "# --- NLTK Downloads (run once at start) ---\n",
    "print(\"Checking NLTK resources...\")\n",
    "try: STOPWORDS = set(stopwords.words('english'))\n",
    "except LookupError: nltk.download('stopwords', quiet=True); STOPWORDS = set(stopwords.words('english'))\n",
    "try: nltk.data.find('corpora/wordnet');\n",
    "except LookupError: nltk.download('wordnet', quiet=True)\n",
    "print(\"NLTK resources ready.\")\n",
    "\n",
    "\n",
    "# --- Define Helper Functions ---\n",
    "\n",
    "def tokenize_and_lemmatize(doc):\n",
    "    \"\"\"Tokenizes, removes stopwords, lemmatizes.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    if doc is None or not isinstance(doc, str) or doc.strip() == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        text = doc.lower()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        tokens = re.split(r'\\s+', text)\n",
    "        lemmatized_tokens = [\n",
    "            lemmatizer.lemmatize(w) for w in tokens\n",
    "            if w not in STOPWORDS and len(w) > 2\n",
    "        ]\n",
    "        return lemmatized_tokens\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "\n",
    "def sample_partition(partition, K_local, alpha_local, beta_local, V_local,\n",
    "                     n_kv_broadcast_local, n_k_broadcast_local, n_d_broadcast_local):\n",
    "    \"\"\"Performs Gibbs sampling update for a partition.\"\"\"\n",
    "    # NOTE: Ensure numpy and random are available/imported in the execution environment\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    current_n_kv = n_kv_broadcast_local.value\n",
    "    current_n_k = n_k_broadcast_local.value\n",
    "    current_n_d = n_d_broadcast_local.value\n",
    "\n",
    "    local_partition_list = list(partition)\n",
    "    if not local_partition_list: return iter([])\n",
    "\n",
    "    local_n_dk = {}\n",
    "    for doc_id, word_id, topic in local_partition_list:\n",
    "        key = (doc_id, topic)\n",
    "        local_n_dk[key] = local_n_dk.get(key, 0) + 1\n",
    "\n",
    "    results = []\n",
    "    K_alpha_term = K_local * alpha_local\n",
    "    V_beta_term = V_local * beta_local\n",
    "    epsilon = 1e-9\n",
    "\n",
    "    for doc_id, word_id, old_topic in local_partition_list:\n",
    "        local_n_dk[(doc_id, old_topic)] -= 1\n",
    "        nd = current_n_d.get(doc_id, 0) - 1\n",
    "        nd = max(0, nd)\n",
    "\n",
    "        probabilities = np.zeros(K_local)\n",
    "        term1_den = nd + K_alpha_term\n",
    "\n",
    "        for k in range(K_local):\n",
    "            ndk = local_n_dk.get((doc_id, k), 0)\n",
    "            nkv = current_n_kv.get((word_id, k), 0)\n",
    "            nk = current_n_k.get(k, 0)\n",
    "\n",
    "            term1 = (ndk + alpha_local) / term1_den if term1_den > 0 else 0\n",
    "            term2_den = nk + V_beta_term\n",
    "            term2 = (nkv + beta_local) / term2_den if term2_den > 0 else 0\n",
    "            probabilities[k] = term1 * term2\n",
    "\n",
    "        prob_sum = np.sum(probabilities)\n",
    "        if prob_sum <= epsilon:\n",
    "            new_topic = random.randint(0, K_local - 1)\n",
    "        else:\n",
    "            normalized_probs = probabilities / prob_sum\n",
    "            if abs(normalized_probs.sum() - 1.0) > 1e-6 :\n",
    "                 normalized_probs /= normalized_probs.sum()\n",
    "            try:\n",
    "                new_topic = np.random.choice(K_local, p=normalized_probs)\n",
    "            except ValueError as e:\n",
    "                 new_topic = random.randint(0, K_local - 1)\n",
    "\n",
    "        local_n_dk[(doc_id, new_topic)] = local_n_dk.get((doc_id, new_topic), 0) + 1\n",
    "        results.append((doc_id, word_id, new_topic))\n",
    "\n",
    "    return iter(results)\n",
    "\n",
    "\n",
    "def run_lda_gibbs(doc_word_tokens_rdd_local, K_local, alpha_local, beta_local, iterations_local, V_local, N_local, spark_context):\n",
    "    \"\"\"Runs LDA Gibbs sampling and returns the execution time.\"\"\"\n",
    "    print(f\"  Running LDA Gibbs: K={K_local}, alpha={alpha_local:.3f}, beta={beta_local}, iters={iterations_local}\")\n",
    "    lda_internal_start_time = time.time()\n",
    "\n",
    "    print(\"    Initializing random topics...\")\n",
    "    doc_word_topic_rdd = doc_word_tokens_rdd_local.map(\n",
    "        lambda x: (x[0], x[1], random.randint(0, K_local - 1))\n",
    "    ).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    print(\"    Calculating initial counts...\")\n",
    "    n_kv_rdd = doc_word_topic_rdd.map(lambda x: ((x[1], x[2]), 1)).reduceByKey(lambda a, b: a + b)\n",
    "    n_k_rdd = n_kv_rdd.map(lambda x: (x[0][1], x[1])).reduceByKey(lambda a, b: a + b)\n",
    "    n_d_rdd = doc_word_topic_rdd.map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    # Collect initial counts (can be large, might need optimization for huge datasets)\n",
    "    try:\n",
    "        n_kv_map = n_kv_rdd.collectAsMap()\n",
    "        n_k_map = n_k_rdd.collectAsMap()\n",
    "        n_d_map = n_d_rdd.collectAsMap()\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR collecting initial maps: {e}\")\n",
    "        # Clean up partially created RDDs and return NaN or raise\n",
    "        doc_word_topic_rdd.unpersist()\n",
    "        raise # Re-raise the exception\n",
    "\n",
    "    n_kv_broadcast = spark_context.broadcast(n_kv_map)\n",
    "    n_k_broadcast = spark_context.broadcast(n_k_map)\n",
    "    n_d_broadcast = spark_context.broadcast(n_d_map)\n",
    "    print(f\"    Initial counts collected and broadcasted (n_kv: {len(n_kv_map)}, n_k: {len(n_k_map)}, n_d: {len(n_d_map)})\")\n",
    "    sys.stdout.flush() # Force print output\n",
    "\n",
    "    broadcast_history = [(n_kv_broadcast, n_k_broadcast)]\n",
    "\n",
    "    print(f\"    Starting {iterations_local} Gibbs sampling iterations...\")\n",
    "    loop_start_time = time.time()\n",
    "    for i in range(iterations_local):\n",
    "        iter_start_time = time.time()\n",
    "        current_n_kv_broadcast, current_n_k_broadcast = broadcast_history[-1]\n",
    "\n",
    "        new_doc_word_topic_rdd = doc_word_topic_rdd.mapPartitions(\n",
    "            lambda p: sample_partition(p, K_local, alpha_local, beta_local, V_local,\n",
    "                                       current_n_kv_broadcast,\n",
    "                                       current_n_k_broadcast,\n",
    "                                       n_d_broadcast)\n",
    "        ).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        new_n_kv_rdd = new_doc_word_topic_rdd.map(lambda x: ((x[1], x[2]), 1)).reduceByKey(lambda a, b: a + b)\n",
    "        new_n_k_rdd = new_n_kv_rdd.map(lambda x: (x[0][1], x[1])).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "        # Collect new maps (can fail here too)\n",
    "        try:\n",
    "             new_n_kv_map = new_n_kv_rdd.collectAsMap()\n",
    "             new_n_k_map = new_n_k_rdd.collectAsMap()\n",
    "        except Exception as e:\n",
    "             print(f\"    ERROR collecting maps in iteration {i+1}: {e}\")\n",
    "             # Cleanup RDDs from this iteration and raise\n",
    "             new_doc_word_topic_rdd.unpersist()\n",
    "             doc_word_topic_rdd.unpersist() # Previous iteration's RDD\n",
    "             # Destroy all broadcasts created so far\n",
    "             n_d_broadcast.destroy(blocking=False)\n",
    "             for kv_b, k_b in broadcast_history:\n",
    "                 try: kv_b.destroy(blocking=False); k_b.destroy(blocking=False)\n",
    "                 except: pass\n",
    "             raise\n",
    "\n",
    "        n_kv_broadcast = spark_context.broadcast(new_n_kv_map)\n",
    "        n_k_broadcast = spark_context.broadcast(new_n_k_map)\n",
    "        broadcast_history.append((n_kv_broadcast, n_k_broadcast))\n",
    "\n",
    "        old_rdd_to_unpersist = doc_word_topic_rdd\n",
    "        doc_word_topic_rdd = new_doc_word_topic_rdd\n",
    "        old_rdd_to_unpersist.unpersist()\n",
    "\n",
    "        iter_duration = time.time() - iter_start_time\n",
    "        if (i + 1) % 20 == 0 or i == 0 or i == iterations_local - 1 : # Print less often\n",
    "             print(f\"      Iter {i+1}/{iterations_local} ({iter_duration:.2f}s). n_kv size: {len(new_n_kv_map)}\")\n",
    "             sys.stdout.flush() # Force print output\n",
    "\n",
    "    loop_duration = time.time() - loop_start_time\n",
    "    print(f\"    Gibbs loop finished in {loop_duration:.2f} seconds.\")\n",
    "\n",
    "    # --- Phi calculation (optional for timing, but good practice) ---\n",
    "    # final_n_kv_broadcast, final_n_k_broadcast = broadcast_history[-1]\n",
    "    # final_n_kv = final_n_kv_broadcast.value\n",
    "    # final_n_k = final_n_k_broadcast.value\n",
    "    # phi_dist = {} # ... calculate phi ...\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"    Cleaning up LDA RDDs and broadcasts...\")\n",
    "    doc_word_topic_rdd.unpersist()\n",
    "    n_d_broadcast.destroy(blocking=False)\n",
    "    for kv_b, k_b in broadcast_history:\n",
    "         try: kv_b.destroy(blocking=False); k_b.destroy(blocking=False)\n",
    "         except: pass # Ignore errors during cleanup\n",
    "\n",
    "    lda_internal_duration = time.time() - lda_internal_start_time\n",
    "    print(f\"  LDA Gibbs run completed in {lda_internal_duration:.2f} seconds.\")\n",
    "    return lda_internal_duration # Return the time taken\n",
    "\n",
    "\n",
    "# --- Storage for results ---\n",
    "executor_timing_results = {}\n",
    "\n",
    "# --- Loop for Executor Scaling ---\n",
    "for core_count in CORE_COUNTS_TO_TEST:\n",
    "    print(f\"\\n===== Testing with {core_count} Core(s) =====\")\n",
    "    test_start_time = time.time()\n",
    "    spark = None # Ensure spark is reset\n",
    "    prep_success = False\n",
    "    lda_success = False\n",
    "\n",
    "    try:\n",
    "        # --- Create/Restart Spark Session ---\n",
    "        # Stop previous session if it exists\n",
    "        if 'spark' in locals() and spark and spark.sparkContext._jsc is not None:\n",
    "            print(\"  Stopping previous Spark session...\")\n",
    "            spark.stop()\n",
    "            time.sleep(3) # Pause to ensure resources are released\n",
    "\n",
    "        master_config = f\"local[{core_count}]\"\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(f\"LDA Executor Test - {core_count} Cores\") \\\n",
    "            .config(\"spark.master\", master_config) \\\n",
    "            .config(\"spark.driver.memory\", SPARK_MEMORY) \\\n",
    "            .config(\"spark.executor.memory\", SPARK_MEMORY) \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .getOrCreate()\n",
    "        print(f\"  Spark Session created for {master_config}\")\n",
    "        spark.sparkContext.setLogLevel(\"WARN\") # Reduce log verbosity\n",
    "\n",
    "        # --- Re-run FULL Preprocessing ---\n",
    "        print(\"  Preprocessing full dataset...\")\n",
    "        prep_start_time = time.time()\n",
    "        df = spark.read.option(\"header\", True).option(\"inferSchema\", True).option(\"multiLine\", True).option(\"escape\", '\\\"').csv(DATA_FILE_PATH) \\\n",
    "            .filter(col(\"Text\").isNotNull() & (col(\"Text\") != \"\")) \\\n",
    "            .withColumn(\"doc_id\", monotonically_increasing_id())\n",
    "        df.cache()  # Cache DataFrame to avoid recomputation\n",
    "        N_full = df.count()\n",
    "        \n",
    "        tokenize_udf = udf(tokenize_and_lemmatize, ArrayType(StringType()))  # Redefine UDF for new session if needed\n",
    "        \n",
    "        tokenized_rdd = df.select(\"doc_id\", tokenize_udf(col(\"Text\")).alias(\"tokens\")) \\\n",
    "                          .rdd.map(lambda row: (row.doc_id, row.tokens))\n",
    "        tokenized_rdd.persist(StorageLevel.MEMORY_AND_DISK).count()\n",
    "\n",
    "        doc_unique_words_rdd = tokenized_rdd.mapValues(lambda words: list(set(words))).cache()\n",
    "        word_doc_counts_rdd = doc_unique_words_rdd.flatMap(lambda x: [(word, 1) for word in x[1]])\n",
    "        word_doc_freq_rdd = word_doc_counts_rdd.reduceByKey(lambda a, b: a + b).cache()\n",
    "\n",
    "        max_doc_count = N_full * MAX_DF_RATIO\n",
    "        filtered_word_doc_freq_rdd = word_doc_freq_rdd.filter(\n",
    "            lambda wc: wc[1] >= MIN_DF and wc[1] <= max_doc_count\n",
    "        )\n",
    "\n",
    "        filtered_vocabulary_rdd = filtered_word_doc_freq_rdd.map(lambda x: x[0]).zipWithIndex()\n",
    "        V_full = filtered_vocabulary_rdd.count()\n",
    "        print(f\"    Full Vocab Size (V_full): {V_full}\")\n",
    "        if V_full == 0: raise ValueError(\"Full dataset vocabulary empty!\")\n",
    "\n",
    "        vocab_map = filtered_vocabulary_rdd.collectAsMap()\n",
    "        vocab_broadcast = spark.sparkContext.broadcast(vocab_map)\n",
    "\n",
    "        filtered_tokenized_rdd = tokenized_rdd.mapValues(\n",
    "            lambda tokens: [token for token in tokens if token in vocab_broadcast.value]\n",
    "        ).cache()\n",
    "\n",
    "        def get_filtered_word_id(token): return vocab_broadcast.value.get(token, -1)\n",
    "        get_filtered_word_id_udf = udf(get_filtered_word_id, IntegerType())\n",
    "\n",
    "        temp_word_counts_rdd = filtered_tokenized_rdd.flatMap(\n",
    "            lambda x: [( (x[0], token), 1 ) for token in x[1]]\n",
    "        ).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "        word_counts_df_filtered = temp_word_counts_rdd.map(\n",
    "            lambda x: Row(doc_id=x[0][0], token=x[0][1], local_count=x[1])\n",
    "        ).toDF().withColumn(\"word_id\", get_filtered_word_id_udf(col(\"token\"))) \\\n",
    "               .select(\"doc_id\", \"word_id\", \"local_count\") \\\n",
    "               .filter(col(\"word_id\") != -1)\n",
    "        word_counts_df_filtered.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        doc_word_tokens_rdd = word_counts_df_filtered.rdd.flatMap(\n",
    "            lambda row: [(row.doc_id, row.word_id)] * row.local_count\n",
    "        )\n",
    "        doc_word_tokens_rdd.persist(StorageLevel.MEMORY_AND_DISK).count()\n",
    "\n",
    "        print(f\"  Preprocessing took {time.time() - prep_start_time:.2f} seconds.\")\n",
    "        prep_success = True\n",
    "\n",
    "        # --- Time LDA ---\n",
    "        print(f\"  Running LDA with {core_count} core(s)...\")\n",
    "        lda_time = run_lda_gibbs(doc_word_tokens_rdd, BEST_K, BEST_ALPHA, BEST_BETA,\n",
    "                                 BEST_ITERATIONS, V_full, N_full, spark.sparkContext)\n",
    "        executor_timing_results[core_count] = lda_time\n",
    "        lda_success = True\n",
    "        print(f\"  LDA Run Time for {core_count} core(s): {lda_time:.2f} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!!!! ERROR processing core count {core_count}: {e} !!!!!\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        executor_timing_results[core_count] = float('nan') # Mark as failed\n",
    "    finally:\n",
    "        # --- Cleanup for this run ---\n",
    "        print(f\"  Cleaning up resources for {core_count} core(s)...\")\n",
    "        # Use try-except for safety as RDDs might not exist if prep failed\n",
    "        if prep_success: # Only unpersist/destroy if prep seemed okay\n",
    "            try: tokenized_rdd.unpersist(); print(\"    Unpersisted tokenized_rdd\")\n",
    "            except: pass\n",
    "            try: doc_unique_words_rdd.unpersist(); print(\"    Unpersisted doc_unique_words_rdd\")\n",
    "            except: pass\n",
    "            try: word_doc_freq_rdd.unpersist(); print(\"    Unpersisted word_doc_freq_rdd\")\n",
    "            except: pass\n",
    "            try: vocab_broadcast.destroy(blocking=False); print(\"    Destroyed vocab_broadcast\")\n",
    "            except: pass\n",
    "            try: filtered_tokenized_rdd.unpersist(); print(\"    Unpersisted filtered_tokenized_rdd\")\n",
    "            except: pass\n",
    "            try: word_counts_df_filtered.unpersist(); print(\"    Unpersisted word_counts_df_filtered\")\n",
    "            except: pass\n",
    "            try: doc_word_tokens_rdd.unpersist(); print(\"    Unpersisted doc_word_tokens_rdd\")\n",
    "            except: pass\n",
    "        # Stop spark session for this run\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "            print(\"    Stopped Spark session.\")\n",
    "            spark = None # Ensure it's seen as stopped\n",
    "            time.sleep(3) # Pause\n",
    "\n",
    "        print(f\"===== Finished testing {core_count} core(s) in {time.time() - test_start_time:.2f} seconds =====\")\n",
    "        sys.stdout.flush() # Force print output\n",
    "\n",
    "\n",
    "# --- Analyze and Plot Executor Scaling Results ---\n",
    "print(\"\\n--- Executor Scaling Results (Time vs. Cores) ---\")\n",
    "if executor_timing_results:\n",
    "    # Filter out failed runs before calculating speedup/plotting\n",
    "    valid_cores = sorted([c for c, t in executor_timing_results.items() if not math.isnan(t)])\n",
    "    valid_times = [executor_timing_results[c] for c in valid_cores]\n",
    "\n",
    "    print(\"Cores vs. LDA Time:\")\n",
    "    for cores, t in zip(valid_cores, valid_times):\n",
    "        print(f\"  Cores={cores}: {t:.2f} seconds\")\n",
    "\n",
    "    # Calculate Speedup (relative to 1 core time if available)\n",
    "    if 1 in valid_cores:\n",
    "        time_1_core = executor_timing_results[1]\n",
    "        speedup = [time_1_core / t if t > 0 else 0 for t in valid_times]\n",
    "        print(\"\\nCores vs. Speedup:\")\n",
    "        for cores, sp in zip(valid_cores, speedup):\n",
    "            print(f\"  Cores={cores}: {sp:.2f}x\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.subplot(1, 2, 1) # Plot time vs cores\n",
    "        plt.plot(valid_cores, valid_times, marker='o')\n",
    "        plt.xlabel(\"Number of Executor Cores\")\n",
    "        plt.ylabel(\"LDA Execution Time (seconds)\")\n",
    "        plt.title(\"LDA Time vs. Cores\")\n",
    "        plt.xticks(valid_cores)\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1, 2, 2) # Plot speedup vs cores\n",
    "        plt.plot(valid_cores, speedup, marker='o', label='Actual Speedup')\n",
    "        plt.plot(valid_cores, valid_cores, linestyle='--', color='grey', label='Ideal Speedup') # Ideal linear speedup\n",
    "        plt.xlabel(\"Number of Executor Cores\")\n",
    "        plt.ylabel(\"Speedup (T_1 / T_N)\")\n",
    "        plt.title(\"LDA Speedup vs. Cores\")\n",
    "        plt.xticks(valid_cores)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.suptitle(f\"Executor Scaling (K={BEST_K}, N={N_full}, iters={BEST_ITERATIONS})\")\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout\n",
    "        plt.savefig(f\"Executor_Scaling_medium_K_19.png\")\n",
    "        plt.close()\n",
    "\n",
    "    elif valid_cores: # If 1 core run failed/missing, just plot time\n",
    "        print(\"\\nCannot calculate speedup because 1-core run result is missing or invalid.\")\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        plt.plot(valid_cores, valid_times, marker='o')\n",
    "        plt.xlabel(\"Number of Executor Cores\")\n",
    "        plt.ylabel(\"LDA Execution Time (seconds)\")\n",
    "        plt.title(f\"LDA Time vs. Cores (K={BEST_K}, N={N_full})\")\n",
    "        plt.xticks(valid_cores)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"Executor_Scaling_medium_K_19.png\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"No valid executor scaling results to plot.\")\n",
    "\n",
    "else:\n",
    "    print(\"No executor scaling results recorded.\")\n",
    "\n",
    "print(\"--- End Executor Scaling Experiment ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb1a28-8b77-4222-b644-980102fb391c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
